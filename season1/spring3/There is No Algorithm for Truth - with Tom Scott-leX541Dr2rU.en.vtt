WEBVTT
Kind: captions
Language: en

00:00:10.240 --> 00:00:12.700
Imagine that tomorrow,
Google announces

00:00:12.700 --> 00:00:15.160
that they have invented
a machine learning system

00:00:15.160 --> 00:00:18.580
that can tell fact from fiction,
that can determine truth

00:00:18.580 --> 00:00:19.530
from lie.

00:00:19.530 --> 00:00:22.340
And this is a magical
psychology land experiment.

00:00:22.340 --> 00:00:24.700
So in this experiment, in
this thought experiment,

00:00:24.700 --> 00:00:28.098
the machine learning system
is right all the time.

00:00:28.098 --> 00:00:29.890
And then they say that
they've hooked it up

00:00:29.890 --> 00:00:32.340
to the Google search results
and YouTube recommendations,

00:00:32.340 --> 00:00:35.920
so now, when you search
for vaccinations,

00:00:35.920 --> 00:00:37.790
only the scientific
consensus comes up.

00:00:37.790 --> 00:00:40.472
All the anti-vax stuff is
down page six, page seven,

00:00:40.472 --> 00:00:41.680
where no one's going to look.

00:00:41.680 --> 00:00:43.960
YouTube videos on
conspiracy theories

00:00:43.960 --> 00:00:46.250
start to just be recommended
less and less and less

00:00:46.250 --> 00:00:49.120
and the ones debunking them
keep coming up at the top.

00:00:49.120 --> 00:00:51.700
It still matters whether the
search result is engaging

00:00:51.700 --> 00:00:54.100
and relevant and
well-referenced,

00:00:54.100 --> 00:00:58.400
but now it also matters,
objectively, whether it's true.

00:00:58.400 --> 00:01:01.540
And then you realise that this
new system, this algorithm,

00:01:01.540 --> 00:01:04.910
it disagrees with you on
something really important,

00:01:04.910 --> 00:01:09.320
and it produces a result
that you find abhorrent.

00:01:09.320 --> 00:01:12.040
Now, I struggle to
find an emotive example

00:01:12.040 --> 00:01:14.540
for this that wouldn't also be
too sensitive for an audience

00:01:14.540 --> 00:01:18.980
like this, so as this is the
Royal Institution in London

00:01:18.980 --> 00:01:22.700
and it is September 2019,
imagine this machine learning

00:01:22.700 --> 00:01:24.770
system concludes that
it would be a good idea

00:01:24.770 --> 00:01:26.180
to have a no deal Brexit.

00:01:26.180 --> 00:01:29.006
[LAUGHTER]

00:01:31.361 --> 00:01:33.470
OK, now that-- that
was-- don't applaud.

00:01:33.470 --> 00:01:34.603
That was a cheap shot.

00:01:34.603 --> 00:01:35.520
That was a cheap shot.

00:01:35.520 --> 00:01:39.050
And there may be people in here
who genuinely believe that.

00:01:39.050 --> 00:01:41.330
So for those people
please imagine

00:01:41.330 --> 00:01:44.720
the extreme opposite conclusion,
that this system believes

00:01:44.720 --> 00:01:47.330
it will be a good idea
to dissolve the United

00:01:47.330 --> 00:01:50.390
Kingdom into Europe entirely.

00:01:50.390 --> 00:01:53.150
Look, this is a system that,
in this thought experiment,

00:01:53.150 --> 00:01:55.250
does create objective
truth, and it

00:01:55.250 --> 00:01:59.570
disagrees with one of your
fundamental core values.

00:01:59.570 --> 00:02:02.150
What's more likely,
A, that you are

00:02:02.150 --> 00:02:04.760
going to just go,
oh, well, OK, I guess

00:02:04.760 --> 00:02:06.350
I'm going to be
heard less and less,

00:02:06.350 --> 00:02:08.880
and I guess we'll just have
to deal with that, or B,

00:02:08.880 --> 00:02:12.620
that you decide that instead,
you need to shout louder

00:02:12.620 --> 00:02:15.200
and that the algorithm is wrong?

00:02:15.200 --> 00:02:17.200
So for the next
hour I want to talk

00:02:17.200 --> 00:02:19.510
about the state of play
for science communication

00:02:19.510 --> 00:02:21.730
and broadcast
communication in general

00:02:21.730 --> 00:02:24.430
for an audience that has
wildly different ideas

00:02:24.430 --> 00:02:27.250
and areas of knowledge on
how that works right now.

00:02:27.250 --> 00:02:29.950
I want to talk about why some
science communication seems

00:02:29.950 --> 00:02:33.940
to go everywhere and a lot of
it doesn't, and to give advice

00:02:33.940 --> 00:02:36.310
for people who are
trying to reach out

00:02:36.310 --> 00:02:39.730
and to broadcast their
truth, and whether that's

00:02:39.730 --> 00:02:43.450
for small, individual
groups putting out content,

00:02:43.450 --> 00:02:46.390
or whether it's big
corporations and charities who

00:02:46.390 --> 00:02:47.482
are trying to do outreach.

00:02:47.482 --> 00:02:48.940
That's also going
to include a dive

00:02:48.940 --> 00:02:51.440
into the concept of
parasocial relationships,

00:02:51.440 --> 00:02:53.890
those odd, one-way
relationships that

00:02:53.890 --> 00:02:55.215
appear in other media a lot.

00:02:55.215 --> 00:02:56.590
Because if you
want to understand

00:02:56.590 --> 00:02:58.990
how to reach an
audience, then you

00:02:58.990 --> 00:03:02.178
need to understand what that
audience is looking for.

00:03:02.178 --> 00:03:03.970
And I want to talk
about one particular set

00:03:03.970 --> 00:03:07.510
of things that are increasingly
governing the media we consume

00:03:07.510 --> 00:03:10.660
and everything in our
lives, algorithms,

00:03:10.660 --> 00:03:13.420
the ones that recommend the
videos you watch on YouTube,

00:03:13.420 --> 00:03:16.120
the search results on Google,
the order that your Twitter

00:03:16.120 --> 00:03:19.030
feed appears in, and
basically everything that

00:03:19.030 --> 00:03:22.320
includes recommendations
on social media these days.

00:03:22.320 --> 00:03:24.610
And from the corporation's
point of view,

00:03:24.610 --> 00:03:26.530
which advertising to show you.

00:03:26.530 --> 00:03:28.270
Now, there are a
couple of caveats here.

00:03:28.270 --> 00:03:31.660
I have generalised this talk
as much as I possibly can.

00:03:31.660 --> 00:03:34.070
I've run it past quite a
few folks in my industry,

00:03:34.070 --> 00:03:37.270
but I am speaking from
a position of success.

00:03:37.270 --> 00:03:41.327
I'm lucky enough to have 1.9
million subscribers on YouTube

00:03:41.327 --> 00:03:41.910
at the minute.

00:03:41.910 --> 00:03:42.790
There we go.

00:03:42.790 --> 00:03:44.997
Didn't quite get to
2 million in time.

00:03:44.997 --> 00:03:47.620
[LAUGHTER]

00:03:47.620 --> 00:03:50.922
Now, subscribers is not a
particularly useful metric.

00:03:50.922 --> 00:03:52.630
That's more a function
of how long you've

00:03:52.630 --> 00:03:54.005
been on the platform
and how many

00:03:54.005 --> 00:03:55.600
one hit wonders you've had.

00:03:55.600 --> 00:03:58.450
It's more honest to say that an
average science communication

00:03:58.450 --> 00:04:01.840
video from me gets somewhere
between a quarter of a million

00:04:01.840 --> 00:04:02.830
and a million views.

00:04:02.830 --> 00:04:04.747
And those might be about
linguistics, which is

00:04:04.747 --> 00:04:06.550
what my degree is actually in.

00:04:06.550 --> 00:04:08.677
They might be about the
basics of computer science,

00:04:08.677 --> 00:04:10.510
where I'm self-taught
but checking my script

00:04:10.510 --> 00:04:11.680
with experts.

00:04:11.680 --> 00:04:14.470
Or they might be about
infrastructure and science

00:04:14.470 --> 00:04:16.839
and interesting things
in the world, which

00:04:16.839 --> 00:04:19.300
is where I go out
on location and I

00:04:19.300 --> 00:04:22.730
hand over to people who know
what they're talking about.

00:04:22.730 --> 00:04:25.310
Now, my degree is in linguistics
with a research master's

00:04:25.310 --> 00:04:27.280
in Educational Studies,
but ultimately, I

00:04:27.280 --> 00:04:29.240
am in this position
because I spent

00:04:29.240 --> 00:04:33.300
15 years throwing things at
the internet before something

00:04:33.300 --> 00:04:33.800
worked.

00:04:33.800 --> 00:04:37.130
I was extremely lucky that the
thing that turned out to work

00:04:37.130 --> 00:04:40.340
was science communication, was
going out and telling the world

00:04:40.340 --> 00:04:41.870
about things I'm interested in.

00:04:41.870 --> 00:04:43.700
I am even luckier
that it turned out

00:04:43.700 --> 00:04:45.740
to involve filming on location.

00:04:45.740 --> 00:04:47.840
In the past few years,
I've been lucky enough

00:04:47.840 --> 00:04:50.090
to experience zero gravity.

00:04:50.090 --> 00:04:51.740
I have gone to the Arctic.

00:04:51.740 --> 00:04:54.470
I have flown with
the Red Arrows.

00:04:54.470 --> 00:04:57.215
And yes, that is mostly
a brag, and mostly--

00:04:57.215 --> 00:04:57.950
[LAUGHTER]

00:04:57.950 --> 00:05:00.380
--just an excuse to show
the best photo of me that

00:05:00.380 --> 00:05:02.030
will ever be taken in my life.

00:05:02.030 --> 00:05:04.113
[LAUGHTER]

00:05:04.113 --> 00:05:05.780
Have you ever lost a
photo of yourselves

00:05:05.780 --> 00:05:07.700
and thought, it's all
downhill from here?

00:05:07.700 --> 00:05:10.910
Because it is.

00:05:10.910 --> 00:05:11.848
One more caveat.

00:05:11.848 --> 00:05:14.390
For those of you who've been to
a Royal Institution discourse

00:05:14.390 --> 00:05:16.473
before, you'll know there
are generally two types.

00:05:16.473 --> 00:05:19.190
There was one where a
researcher with a PhD

00:05:19.190 --> 00:05:22.550
and an associate professorship
talks about their research,

00:05:22.550 --> 00:05:25.250
and there is one where
someone in arts and culture

00:05:25.250 --> 00:05:26.700
shares their experience.

00:05:26.700 --> 00:05:28.790
This is more of the latter.

00:05:28.790 --> 00:05:31.220
Some of what I say is going
to be opinion and not fact,

00:05:31.220 --> 00:05:33.890
and hopefully this audience will
be able to tell the difference.

00:05:33.890 --> 00:05:36.740
There are points in here
where I explicitly say,

00:05:36.740 --> 00:05:39.020
I do not have all the answers.

00:05:39.020 --> 00:05:41.480
And I also want to add one
conflict of interest note

00:05:41.480 --> 00:05:42.440
as well.

00:05:42.440 --> 00:05:45.260
My company gets a
lot of its revenue

00:05:45.260 --> 00:05:49.070
from the adverts that go on and
around YouTube videos, which

00:05:49.070 --> 00:05:53.632
means that my rent is indirectly
paid by Google Ireland.

00:05:53.632 --> 00:05:55.760
I can't imagine why
it's in Ireland.

00:05:55.760 --> 00:06:00.110
No-- no reason at all why they
set up there instead of the UK.

00:06:00.110 --> 00:06:02.390
Not a word of this
discourse has been

00:06:02.390 --> 00:06:03.830
passed by anyone in Google.

00:06:03.830 --> 00:06:07.400
They don't even
know I'm doing it.

00:06:07.400 --> 00:06:10.700
I'm not employed by them,
but ultimately, like, I am--

00:06:10.700 --> 00:06:13.370
while I'm willing to
irritate that company

00:06:13.370 --> 00:06:15.830
and bite the hand
that feeds me, they

00:06:15.830 --> 00:06:17.380
are indirectly paying my rent.

00:06:17.380 --> 00:06:19.880
I can try to represent the folks
who have not been as lucky,

00:06:19.880 --> 00:06:21.980
who the algorithm
has turned against,

00:06:21.980 --> 00:06:25.165
but I am quite happy
with them right now,

00:06:25.165 --> 00:06:26.540
and a lot of other
people aren't.

00:06:26.540 --> 00:06:27.920
So anyway, that's the plan.

00:06:27.920 --> 00:06:31.160
This is the state of
science communication

00:06:31.160 --> 00:06:35.330
in the English-speaking world
at the end of the second decade

00:06:35.330 --> 00:06:36.923
of the 21st century.

00:06:36.923 --> 00:06:38.340
And to understand
how it works, we

00:06:38.340 --> 00:06:42.080
need to start with
the algorithm.

00:06:42.080 --> 00:06:45.290
Algorithm, in this
context, means quite a bit

00:06:45.290 --> 00:06:48.260
different from what people
with a lot of experience

00:06:48.260 --> 00:06:51.380
in mathematics and computer
science might think it does.

00:06:51.380 --> 00:06:54.050
The algorithm is referred
to in the singular, which

00:06:54.050 --> 00:06:57.830
is the almost
anthropomorphized name that's

00:06:57.830 --> 00:07:02.120
given to this collection of
machine learning systems.

00:07:02.120 --> 00:07:05.090
I went to a conference for
science communicators last

00:07:05.090 --> 00:07:09.050
year, and after about
three or four hours,

00:07:09.050 --> 00:07:11.840
we realised that we had to ban
the word from conversation,

00:07:11.840 --> 00:07:15.620
because while a lot of folks
from YouTube would just

00:07:15.620 --> 00:07:19.070
endlessly froth about it, anyone
not on the platform just found

00:07:19.070 --> 00:07:20.540
it confusing and messy.

00:07:20.540 --> 00:07:22.280
And we would not shut up.

00:07:22.280 --> 00:07:23.890
So when I talk
about the algorithm,

00:07:23.890 --> 00:07:28.900
I am talking about this almost
magical black box of code.

00:07:28.900 --> 00:07:31.400
And the idea is that you
set up this black box

00:07:31.400 --> 00:07:35.960
and then you provide it with a
list of human-curated examples,

00:07:35.960 --> 00:07:38.330
and it works out their
distinguishing features,

00:07:38.330 --> 00:07:41.060
provide some sort of
categorization system,

00:07:41.060 --> 00:07:43.150
and then as you throw
novel examples at it,

00:07:43.150 --> 00:07:46.015
it categorises them and
learns from feedback.

00:07:46.015 --> 00:07:47.390
And those
distinguishing features

00:07:47.390 --> 00:07:51.570
may be completely novel or
completely unknown to humans.

00:07:51.570 --> 00:07:55.480
So one of Google's
recent AI projects

00:07:55.480 --> 00:07:57.340
was looking at
retinal photography.

00:07:57.340 --> 00:07:59.720
And this recent paper
claims that with

00:07:59.720 --> 00:08:03.520
or with uncanny accuracy, it was
able to look at retinal photos

00:08:03.520 --> 00:08:05.740
and work out gender--

00:08:05.740 --> 00:08:08.830
sex with 97% accuracy,
age within four years,

00:08:08.830 --> 00:08:11.470
and better than chance
at smoking status,

00:08:11.470 --> 00:08:14.180
blood pressure, major
adverse cardiac events.

00:08:14.180 --> 00:08:18.280
Eye doctors now cannot detect
any of those things themselves.

00:08:18.280 --> 00:08:20.680
They're not really sure
how the machine did it.

00:08:20.680 --> 00:08:23.110
Now, it's right to be sceptical
of some of those claims.

00:08:23.110 --> 00:08:25.390
Maybe there's a difference
in the metadata.

00:08:25.390 --> 00:08:28.270
Maybe the retinal
photography machine--

00:08:28.270 --> 00:08:29.910
I guess that's
technically a camera--

00:08:29.910 --> 00:08:32.110
is set up or focused
differently depending

00:08:32.110 --> 00:08:33.850
on some of those attributes.

00:08:33.850 --> 00:08:35.380
But the paper does
a pretty good job

00:08:35.380 --> 00:08:37.620
of covering their bases there.

00:08:37.620 --> 00:08:39.520
And maybe a human
could also be trained

00:08:39.520 --> 00:08:41.440
to pick out those
differences, it's

00:08:41.440 --> 00:08:43.659
just that nobody's
bothered when you can just

00:08:43.659 --> 00:08:45.970
look at the chart
next to the patient.

00:08:45.970 --> 00:08:50.110
But the simplest black box
machine learning system

00:08:50.110 --> 00:08:52.270
is essentially
categorising pictures.

00:08:52.270 --> 00:08:55.275
You give it a load
of pictures of cats

00:08:55.275 --> 00:08:56.650
and you give it
a lot of pictures

00:08:56.650 --> 00:09:00.160
of things that are not
cats, and then you ask it,

00:09:00.160 --> 00:09:02.520
is this new picture a cat?

00:09:02.520 --> 00:09:04.270
Which sounds like it's
going to be useless

00:09:04.270 --> 00:09:07.002
unless you're trying to design
a philtre for adult content

00:09:07.002 --> 00:09:08.710
and you're sending it
pornography and not

00:09:08.710 --> 00:09:11.620
pornography, and the aim is
to have a classifier that

00:09:11.620 --> 00:09:13.750
can look at a photo
it's never seen before

00:09:13.750 --> 00:09:16.860
and work out whether you should
show that to all ages or not.

00:09:16.860 --> 00:09:18.590
Of course, it's not that simple.

00:09:18.590 --> 00:09:21.460
There are stories after
stories after stories

00:09:21.460 --> 00:09:23.440
of machine learning
systems that have

00:09:23.440 --> 00:09:26.650
failed through bad training
data or, more likely,

00:09:26.650 --> 00:09:28.720
biassed training data.

00:09:28.720 --> 00:09:31.300
And this is slightly
outside my ballpark.

00:09:31.300 --> 00:09:33.850
I know Hannah Fry covered
this in her lecture

00:09:33.850 --> 00:09:35.060
here a while ago.

00:09:35.060 --> 00:09:36.770
But I have a very--

00:09:36.770 --> 00:09:38.990
I have an example that's
very close to my heart.

00:09:38.990 --> 00:09:40.990
YouTube uses the
machine learning system

00:09:40.990 --> 00:09:44.830
to try and detect whether videos
are suitable for advertisers

00:09:44.830 --> 00:09:46.780
to place their adverts next to.

00:09:46.780 --> 00:09:49.750
It was rolled out a little
bit too fast, and before it

00:09:49.750 --> 00:09:51.435
was entirely ready,
because YouTube

00:09:51.435 --> 00:09:53.560
had one of the many little
scandals that they have,

00:09:53.560 --> 00:09:57.020
and they needed to do something
to reassure their advertisers.

00:09:57.020 --> 00:10:00.910
So this is a highly
abridged summary

00:10:00.910 --> 00:10:03.670
based on unofficial
conversations and innuendo

00:10:03.670 --> 00:10:04.350
and scuttlebutt.

00:10:04.350 --> 00:10:07.630
I'm breaking no NDAs
here, but the story

00:10:07.630 --> 00:10:10.090
goes that they provided
the machine learning system

00:10:10.090 --> 00:10:12.460
with a big block of videos
that were definitely

00:10:12.460 --> 00:10:16.330
100% safe for
advertisers, and then

00:10:16.330 --> 00:10:18.600
they gave it a big block
that were definitely not.

00:10:18.600 --> 00:10:20.770
And they told the system
to be fairly conservative,

00:10:20.770 --> 00:10:23.030
because it was only a
first line of defence.

00:10:23.030 --> 00:10:25.270
If it deemed your
video unsuitable,

00:10:25.270 --> 00:10:27.760
you could send it
off to humans review.

00:10:27.760 --> 00:10:30.310
And this, is in my industry,
a fairly controversial thing

00:10:30.310 --> 00:10:33.970
to say, but I don't think
that's an unreasonable solution

00:10:33.970 --> 00:10:35.620
to a very difficult problem.

00:10:35.620 --> 00:10:39.900
YouTube has 500
hours of content--

00:10:39.900 --> 00:10:41.020
500 hours of video.

00:10:41.020 --> 00:10:42.190
I'm not saying content.

00:10:42.190 --> 00:10:48.490
YouTube has 500 hours of
video uploaded every minute.

00:10:48.490 --> 00:10:51.640
That's a human lifetime,
every single day.

00:10:51.640 --> 00:10:54.580
It is not unreasonable to have
a machine learning system be

00:10:54.580 --> 00:10:58.090
the first line of defence as
long as there's a human review

00:10:58.090 --> 00:10:58.820
behind it.

00:10:58.820 --> 00:11:00.670
And nowadays, it's
working fairly well

00:11:00.670 --> 00:11:03.160
with some high
profile exceptions,

00:11:03.160 --> 00:11:05.050
but the problem,
so I'm told, was

00:11:05.050 --> 00:11:07.870
that there was a bias
in the training data.

00:11:07.870 --> 00:11:11.200
Videos-- sorry,
channels, people,

00:11:11.200 --> 00:11:13.690
talking about LGBT
issues were more

00:11:13.690 --> 00:11:17.890
likely to talk explicitly about
sex in some of their videos.

00:11:17.890 --> 00:11:18.730
Not all of them.

00:11:18.730 --> 00:11:20.380
Not the majority of them.

00:11:20.380 --> 00:11:22.960
But enough that the machine
learning system figured out

00:11:22.960 --> 00:11:27.610
there was a correlation between
people talking about LGBT stuff

00:11:27.610 --> 00:11:30.790
and people talking
about explicit sex.

00:11:30.790 --> 00:11:32.800
Again, only in a small
number of videos,

00:11:32.800 --> 00:11:36.070
but enough that when the machine
learning system found something

00:11:36.070 --> 00:11:41.770
to be about being gay, it viewed
it as more likely to be unsafe.

00:11:41.770 --> 00:11:46.690
Now, the YouTube CEO said
in a recent interview,

00:11:46.690 --> 00:11:49.013
"We work incredibly
hard to make sure

00:11:49.013 --> 00:11:50.680
that when our machines
learn something--

00:11:50.680 --> 00:11:53.055
because a lot of our decisions
are made algorithmically--

00:11:53.055 --> 00:11:54.980
that our machines are fair."

00:11:54.980 --> 00:11:56.230
I know some YouTube employees.

00:11:56.230 --> 00:11:57.897
I'm friends with some
YouTube employees.

00:11:57.897 --> 00:12:00.520
I believe that they work
incredibly hard to minimise

00:12:00.520 --> 00:12:01.900
that bias.

00:12:01.900 --> 00:12:03.210
But it's still there.

00:12:03.210 --> 00:12:05.350
And algorithmic bias
is a major concern

00:12:05.350 --> 00:12:08.260
for every single
machine learning system.

00:12:08.260 --> 00:12:10.030
The systematic biases
in the wider world

00:12:10.030 --> 00:12:12.760
have already found their
way to social media

00:12:12.760 --> 00:12:14.830
without machine
learning being involved.

00:12:14.830 --> 00:12:18.910
Of the top 10 earning creators
on YouTube right now--

00:12:18.910 --> 00:12:23.140
well, as of 2018, as of
last year, of the top 10,

00:12:23.140 --> 00:12:24.552
all of them are male.

00:12:24.552 --> 00:12:27.010
And I'm well aware that I'm in
the Royal Institution giving

00:12:27.010 --> 00:12:27.790
this talk.

00:12:27.790 --> 00:12:29.657
One of the reasons
that I got the audience

00:12:29.657 --> 00:12:31.240
in the first place,
one of the reasons

00:12:31.240 --> 00:12:33.437
I ended up standing
here, is because I'm

00:12:33.437 --> 00:12:35.020
a white guy with a
British accent that

00:12:35.020 --> 00:12:36.470
sounds authoritative.

00:12:36.470 --> 00:12:36.970
Trying--

00:12:36.970 --> 00:12:37.470
[LAUGHTER]

00:12:37.470 --> 00:12:39.850
Trying to make sure that
artificial intelligence doesn't

00:12:39.850 --> 00:12:44.440
inherit these systemic biases
is an incredibly difficult job.

00:12:44.440 --> 00:12:46.540
And it's one for Hannah
Fry and her crew,

00:12:46.540 --> 00:12:49.017
and not for someone who
got a linguistics degree.

00:12:49.017 --> 00:12:51.820
[LAUGHTER]

00:12:51.820 --> 00:12:55.390
When YouTube handed over
that recommendation engine

00:12:55.390 --> 00:12:58.810
to machine learning, they set
it to increase watch time.

00:12:58.810 --> 00:13:00.070
This is what we told everyone.

00:13:00.070 --> 00:13:02.560
If people stuck around
watching your video all

00:13:02.560 --> 00:13:04.720
the way to the end, and
it was 20 minutes long,

00:13:04.720 --> 00:13:08.110
then it was viewed as good
by the system, at which point

00:13:08.110 --> 00:13:10.210
they fell foul of
Goodhart's law.

00:13:10.210 --> 00:13:11.980
"When a metric
becomes a target, it

00:13:11.980 --> 00:13:14.270
ceases to be a good measure."

00:13:14.270 --> 00:13:16.297
So people made longer
videos, and they

00:13:16.297 --> 00:13:18.130
put all the important
stuff at the very end,

00:13:18.130 --> 00:13:19.963
forcing people to watch
all the way through,

00:13:19.963 --> 00:13:21.670
so worse videos were
being recommended.

00:13:21.670 --> 00:13:24.340
So now YouTube's
official line is

00:13:24.340 --> 00:13:27.160
that they reward high
quality videos that

00:13:27.160 --> 00:13:30.130
keep people on platform.

00:13:30.130 --> 00:13:32.320
Now, that may not be
videos on the same channel

00:13:32.320 --> 00:13:34.300
by the same creator
in the same genre.

00:13:34.300 --> 00:13:37.862
In 2017, Jim McFadden, who was
then technical lead for YouTube

00:13:37.862 --> 00:13:39.820
recommendations, he talked
about the new engine

00:13:39.820 --> 00:13:42.550
they had that came from a
department wonderfully called

00:13:42.550 --> 00:13:45.100
Google Brain.

00:13:45.100 --> 00:13:46.900
"One of the key things
it does," he says,

00:13:46.900 --> 00:13:48.910
"is it's able to generalise.

00:13:48.910 --> 00:13:51.310
Whereas before, if I watched
this video from a comedian,

00:13:51.310 --> 00:13:52.990
our recommendations were
pretty good at saying,

00:13:52.990 --> 00:13:54.370
here's another one just like it.

00:13:54.370 --> 00:13:56.740
But the Google Brain
model figures out

00:13:56.740 --> 00:13:59.230
the other comedians
who are similar

00:13:59.230 --> 00:14:02.860
but not exactly the same-- even
more adjacent relationships.

00:14:02.860 --> 00:14:07.100
It's able to see patterns
that are less obvious."

00:14:07.100 --> 00:14:09.520
And as for which videos
to recommend, well,

00:14:09.520 --> 00:14:11.310
one of the new
model's basic ideas

00:14:11.310 --> 00:14:15.240
is that if someone comes to
YouTube because of your video,

00:14:15.240 --> 00:14:17.440
tick, that's a good thing.

00:14:17.440 --> 00:14:21.150
And if someone does not leave
YouTube because of your video,

00:14:21.150 --> 00:14:25.030
does not stop watching,
that's a good thing.

00:14:25.030 --> 00:14:27.690
So the black box
takes in those signals

00:14:27.690 --> 00:14:30.733
and it works out, what's going
to keep people on our platform?

00:14:30.733 --> 00:14:32.650
What's going to keep
them watching the videos,

00:14:32.650 --> 00:14:35.010
and again, more importantly
for Google, watching

00:14:35.010 --> 00:14:37.430
the adverts in between.

00:14:37.430 --> 00:14:40.010
Instantly, apparently,
Google also

00:14:40.010 --> 00:14:41.510
serves more adverts
to people who

00:14:41.510 --> 00:14:44.261
are more tolerant to adverts.

00:14:44.261 --> 00:14:46.550
[LAUGHTER]

00:14:46.550 --> 00:14:47.600
If you'd like less--

00:14:47.600 --> 00:14:49.310
if you'd like two
adverts to appear

00:14:49.310 --> 00:14:52.184
less often before your
video, skip them more.

00:14:52.184 --> 00:14:54.737
[CHUCKLES] That was bad
advice to give for someone to

00:14:54.737 --> 00:14:55.820
makes his money from that.

00:14:55.820 --> 00:14:57.670
[LAUGHTER]

00:14:57.670 --> 00:15:00.710
Because you've got to remember,
all these big companies,

00:15:00.710 --> 00:15:03.860
Google, Facebook, Twitter,
they are essentially

00:15:03.860 --> 00:15:05.510
advertising companies.

00:15:05.510 --> 00:15:09.560
Almost all their revenue
comes from being the greatest

00:15:09.560 --> 00:15:12.020
marketing advertising
targeting company

00:15:12.020 --> 00:15:13.640
that the world has ever seen.

00:15:13.640 --> 00:15:18.770
Their ideal is that every advert
is perfectly targeted to you.

00:15:18.770 --> 00:15:22.850
And as all of you will be aware,
they haven't got there yet.

00:15:22.850 --> 00:15:25.340
But it turns out if
you reward videos

00:15:25.340 --> 00:15:29.120
that keep people on platform,
then what you end up with

00:15:29.120 --> 00:15:31.360
is conspiracy theories
and clickbait.

00:15:31.360 --> 00:15:34.580
And yes, all, all, of the
companies that have algorithms

00:15:34.580 --> 00:15:36.403
are working on fighting
disinformation,

00:15:36.403 --> 00:15:38.570
because they are aware that
it is a public relations

00:15:38.570 --> 00:15:40.100
disaster for them.

00:15:40.100 --> 00:15:42.875
But they are doing
it in English first.

00:15:42.875 --> 00:15:45.510
I'll get back to that later.

00:15:45.510 --> 00:15:47.970
From a creative
perspective, the algorithm

00:15:47.970 --> 00:15:50.688
is often seen as being a
bit like a Skinner Box.

00:15:50.688 --> 00:15:52.230
It's an operant
conditioning chamber.

00:15:52.230 --> 00:15:54.810
It is a food dispenser
that might give you

00:15:54.810 --> 00:15:57.870
some money if you
tap the lever enough.

00:15:57.870 --> 00:15:59.700
Google and YouTube
and Twitter will never

00:15:59.700 --> 00:16:02.070
tell you what that
algorithm is looking for,

00:16:02.070 --> 00:16:04.582
because every bit of
information they give away

00:16:04.582 --> 00:16:06.540
means that there is more
opportunity for people

00:16:06.540 --> 00:16:08.640
to abuse it and send spam.

00:16:08.640 --> 00:16:11.848
But from my perspective, the
pallets come out at random,

00:16:11.848 --> 00:16:13.890
and we all develop the
superstitions, that we all

00:16:13.890 --> 00:16:16.350
keep pushing the lever.

00:16:16.350 --> 00:16:19.410
I was lucky enough this
year to have a conversation

00:16:19.410 --> 00:16:22.410
with the head of product
for recommendations

00:16:22.410 --> 00:16:26.130
at Google, the person in
charge of the algorithm.

00:16:26.130 --> 00:16:28.440
And it seems like
the idealised version

00:16:28.440 --> 00:16:30.990
of what they want for that,
sort of recommendations that

00:16:30.990 --> 00:16:33.480
appear next to video, is that--

00:16:33.480 --> 00:16:36.810
it's a bit like-- and this is
something that shows my age.

00:16:36.810 --> 00:16:38.640
It's a bit like a TV Guide.

00:16:38.640 --> 00:16:41.250
It's a bit like it's a
bit like the Radio Times.

00:16:41.250 --> 00:16:43.500
Every single channel
should always

00:16:43.500 --> 00:16:47.250
have something on it that
is interesting to you,

00:16:47.250 --> 00:16:48.270
all the time.

00:16:48.270 --> 00:16:51.937
It should be a transparent
glass layer between the audience

00:16:51.937 --> 00:16:53.020
and what they want to see.

00:16:53.020 --> 00:16:56.487
And the question is, as they're
weighing up all those videos,

00:16:56.487 --> 00:16:58.320
how much they put their
finger on the scale?

00:16:58.320 --> 00:17:00.360
How much did they
make that TV Guide

00:17:00.360 --> 00:17:04.770
be for the worthy, honesty,
truth-seeking version of you,

00:17:04.770 --> 00:17:07.089
and not the clickbait
conspiracy version of you?

00:17:07.089 --> 00:17:08.339
Because ultimately,
yes, sometimes you

00:17:08.339 --> 00:17:10.170
want to watch a documentary,
but sometimes you

00:17:10.170 --> 00:17:12.337
want to watch someone trip
over and hurt themselves.

00:17:12.337 --> 00:17:14.430
Like, You've Been Framed
existed for a reason.

00:17:14.430 --> 00:17:16.766
And I'm not just
talking about YouTube.

00:17:16.766 --> 00:17:17.849
I'm talking about Twitter.

00:17:17.849 --> 00:17:21.000
I'm talking about every single
algorithmic system out there.

00:17:21.000 --> 00:17:23.069
If all they're
recommending is quick,

00:17:23.069 --> 00:17:26.410
short dopamine hits that
just get you in, get you out,

00:17:26.410 --> 00:17:29.220
then that's not a long
term survival strategy.

00:17:29.220 --> 00:17:31.690
That spirals down into the
lowest common denominator,

00:17:31.690 --> 00:17:34.450
which ultimately
hurts the world.

00:17:34.450 --> 00:17:36.707
But if they don't have
some of that in there,

00:17:36.707 --> 00:17:38.290
then people are going
to go elsewhere.

00:17:38.290 --> 00:17:41.470
They're going to go to the
company that does have that.

00:17:41.470 --> 00:17:45.940
If everything is painstakingly
verified and educational,

00:17:45.940 --> 00:17:48.940
then only a minority of
folks are going to watch it.

00:17:48.940 --> 00:17:54.998
Finding that balance,
finding that solution--

00:17:54.998 --> 00:17:57.290
I said there were going to
be analogies to older media.

00:17:57.290 --> 00:18:00.370
That's what TV
commissioners still do.

00:18:00.370 --> 00:18:03.130
It's what the people programming
the YouTube algorithm are

00:18:03.130 --> 00:18:03.910
trying to do.

00:18:03.910 --> 00:18:07.180
And let's be clear, it's
an unsolvable problem.

00:18:07.180 --> 00:18:09.095
There is not some
magical equilibrium

00:18:09.095 --> 00:18:10.720
in the middle that
will make this work.

00:18:10.720 --> 00:18:12.940
It's about finding a balance.

00:18:12.940 --> 00:18:16.030
It is about finding
the least worst option.

00:18:16.030 --> 00:18:20.170
You cannot have a successful
platform that is all clickbait,

00:18:20.170 --> 00:18:22.720
but you also can't have a
successful platform with no

00:18:22.720 --> 00:18:26.277
clickbait, because either way,
advertisers are going to leave.

00:18:26.277 --> 00:18:27.610
Viewers are going to tyre of it.

00:18:27.610 --> 00:18:30.510
And it's not
sustainable, long term.

00:18:30.510 --> 00:18:33.630
There have been plenty
of investigations

00:18:33.630 --> 00:18:37.590
into the effects of the
algorithm, plenty of research,

00:18:37.590 --> 00:18:41.940
formal and informal, that showed
how you could very, very easily

00:18:41.940 --> 00:18:46.260
go from something apolitical,
and then click again and find

00:18:46.260 --> 00:18:48.780
something just
slightly political,

00:18:48.780 --> 00:18:51.600
and then click again and then
find something a little bit

00:18:51.600 --> 00:18:54.750
clickbait, but still honest,
and then maybe something that's

00:18:54.750 --> 00:18:58.020
about moderately conservative
politics that's a little bit

00:18:58.020 --> 00:19:00.180
untrue, and then
on the next click,

00:19:00.180 --> 00:19:03.210
find something about why
Hillary Clinton is evil

00:19:03.210 --> 00:19:05.490
and Donald Trump is the
greatest thing to ever happen

00:19:05.490 --> 00:19:07.620
to the universe, or vise versa.

00:19:07.620 --> 00:19:10.440
The most notable,
recent investigation

00:19:10.440 --> 00:19:13.170
into online radicalization
is this deep dive

00:19:13.170 --> 00:19:14.282
by The New York Times.

00:19:14.282 --> 00:19:16.740
And like I say, the companies
are working in English first,

00:19:16.740 --> 00:19:19.170
because The New York Times
looked into radicalization

00:19:19.170 --> 00:19:23.400
in Brazil, and it includes one
of the most sobering paragraphs

00:19:23.400 --> 00:19:24.710
I've seen in a while.

00:19:24.710 --> 00:19:27.990
"Right-wing YouTubers had
hijacked already-viral Zika

00:19:27.990 --> 00:19:29.730
conspiracies, and added a twist.

00:19:29.730 --> 00:19:31.260
Women's rights
groups, they claimed,

00:19:31.260 --> 00:19:33.840
had helped engineer
the virus as an excuse

00:19:33.840 --> 00:19:36.870
to impose mandatory abortions."

00:19:36.870 --> 00:19:40.890
Quoting Zeynep Tufekci, who
was referring to research

00:19:40.890 --> 00:19:42.090
in the Wall Street Journal.

00:19:42.090 --> 00:19:46.200
"Videos about vegetarianism
led to videos about veganism.

00:19:46.200 --> 00:19:48.160
Videos about jogging
led to videos

00:19:48.160 --> 00:19:49.410
about running ultra marathons.

00:19:49.410 --> 00:19:51.900
It seems as if you
are never hardcore

00:19:51.900 --> 00:19:54.240
enough for YouTube's algorithm.

00:19:54.240 --> 00:19:57.270
It promotes, recommends,
and disseminates videos

00:19:57.270 --> 00:19:59.730
in a manner that appears to
constantly up the stakes.

00:19:59.730 --> 00:20:03.030
Given its billion
or so users, YouTube

00:20:03.030 --> 00:20:05.970
may be one of the most powerful
radicalising instruments

00:20:05.970 --> 00:20:09.030
of the 21st century."

00:20:09.030 --> 00:20:14.640
I worry quite a lot about
how complicit I am in that.

00:20:14.640 --> 00:20:18.380
Ben McCowan Wilson, he's
YouTube's UK managing director,

00:20:18.380 --> 00:20:19.860
had an interview.

00:20:19.860 --> 00:20:21.940
And obviously, he
disagrees with that.

00:20:21.940 --> 00:20:24.780
He says that the platform
reduces the spread of content

00:20:24.780 --> 00:20:29.790
designed to mislead people and
raises up authoritative voices.

00:20:29.790 --> 00:20:32.730
Note that he didn't say
that those voices were true

00:20:32.730 --> 00:20:34.620
or that those
voices were correct.

00:20:34.620 --> 00:20:37.820
He said they were authoritative.

00:20:37.820 --> 00:20:39.680
There's an old
Jonathan Swift quote,

00:20:39.680 --> 00:20:41.840
and it took quite
a lot of research

00:20:41.840 --> 00:20:44.390
to prove that this actually
was a Jonathan Swift quote.

00:20:44.390 --> 00:20:47.540
"Falsehood flies, and the
truth comes limping after it."

00:20:47.540 --> 00:20:48.350
Put up your hands.

00:20:48.350 --> 00:20:50.750
Who saw this tweet?

00:20:50.750 --> 00:20:52.635
A few people?

00:20:52.635 --> 00:20:54.510
Yeah, there's about 10
or 12 in the audience.

00:20:54.510 --> 00:20:57.290
I mean, look, it
did great numbers.

00:20:57.290 --> 00:21:00.860
The idea that marmite, you
lay the bottle on the side

00:21:00.860 --> 00:21:01.910
to get that out.

00:21:01.910 --> 00:21:03.800
That did that in great numbers.

00:21:03.800 --> 00:21:07.995
Who here saw the confession
and the retraction?

00:21:07.995 --> 00:21:08.868
[LAUGHTER]

00:21:08.868 --> 00:21:10.410
All right, about
half as many people.

00:21:10.410 --> 00:21:11.880
Good.

00:21:11.880 --> 00:21:13.950
Sure, that's pretty
much harmless.

00:21:13.950 --> 00:21:16.140
By the way, that is
now the Marmite guy.

00:21:16.140 --> 00:21:18.820
That's what he's known as.

00:21:18.820 --> 00:21:21.635
Everyone assumed that that had
been fact-checked by someone,

00:21:21.635 --> 00:21:23.260
because it's not that
important, right?

00:21:23.260 --> 00:21:24.635
But everyone always
assumes that,

00:21:24.635 --> 00:21:27.225
particularly if it agrees
with our preconceptions.

00:21:27.225 --> 00:21:28.600
I mean, the long
term solution is

00:21:28.600 --> 00:21:32.304
to teach information literacy
in schools, but as a--

00:21:32.304 --> 00:21:34.185
[CHUCKLES] as a real
world education policy,

00:21:34.185 --> 00:21:35.560
that's about as
useful as saying,

00:21:35.560 --> 00:21:37.850
we should reduce our
carbon emissions.

00:21:37.850 --> 00:21:39.310
It's true.

00:21:39.310 --> 00:21:40.970
How do we get there?

00:21:40.970 --> 00:21:43.120
I don't have the answer to that.

00:21:43.120 --> 00:21:46.900
Douglas Adams in his novel Dirk
Gently's Holistic Detective

00:21:46.900 --> 00:21:50.740
Agency came up with the idea
of a fictional bit of software

00:21:50.740 --> 00:21:54.550
called reason, which-- it's
a programme which allowed you

00:21:54.550 --> 00:21:58.600
to specify in advance which
decision you wanted it to reach

00:21:58.600 --> 00:22:00.840
and only then give
it all the facts.

00:22:00.840 --> 00:22:02.120
And the programme's job--

00:22:02.120 --> 00:22:04.495
the programme's job, which it
was able to accomplish with

00:22:04.495 --> 00:22:07.570
consummate ease, was simply to
construct a plausible series

00:22:07.570 --> 00:22:10.240
of logical sounding steps
to connect the premises with

00:22:10.240 --> 00:22:11.360
the conclusion.

00:22:11.360 --> 00:22:13.910
In the novel, it was sold to
the Pentagon, highly classified,

00:22:13.910 --> 00:22:16.150
and it explained why they
spent so much on Star Wars.

00:22:16.150 --> 00:22:18.220
Missile defence.

00:22:18.220 --> 00:22:21.730
If you would like to be
convinced of a thing,

00:22:21.730 --> 00:22:24.700
YouTube and Twitter and
all the other networks

00:22:24.700 --> 00:22:27.910
will happily find you
people to convince you.

00:22:27.910 --> 00:22:29.830
If you've lost
your faith in God,

00:22:29.830 --> 00:22:33.310
then there will be happily
dozens of evangelical preachers

00:22:33.310 --> 00:22:36.310
from all sorts of denominations
who will happily bring you back

00:22:36.310 --> 00:22:39.420
into the fold, and
dozens of angry atheists

00:22:39.420 --> 00:22:41.170
who will make sure
you stay out of it.

00:22:41.170 --> 00:22:43.670
Take your pick which
way you want to fall.

00:22:43.670 --> 00:22:46.270
If you want to know
who's to blame for what's

00:22:46.270 --> 00:22:48.940
wrong with the world
right now, then I

00:22:48.940 --> 00:22:50.440
can find you hundreds
of people who

00:22:50.440 --> 00:22:53.710
will get you apoplectically
angry at billionaires

00:22:53.710 --> 00:22:56.080
or immigrants or both.

00:22:56.080 --> 00:22:58.120
Whichever way you
want to go, there

00:22:58.120 --> 00:23:02.990
will be someone authoritative
to tell you about it.

00:23:02.990 --> 00:23:05.800
So how do we define
that authority?

00:23:09.550 --> 00:23:11.285
There used to be gatekeepers.

00:23:11.285 --> 00:23:13.660
Or at least, it seemed like
there used to be gatekeepers.

00:23:13.660 --> 00:23:17.110
Science communication was
in magazines and television

00:23:17.110 --> 00:23:19.210
and radio, and it
meant that there

00:23:19.210 --> 00:23:21.670
wasn't any significant peer
review system out there,

00:23:21.670 --> 00:23:23.830
but you knew that
there were researchers

00:23:23.830 --> 00:23:26.340
at the back end and
standards and quality

00:23:26.340 --> 00:23:27.970
to keep these things
fairly accurate.

00:23:27.970 --> 00:23:29.012
There were professionals.

00:23:29.012 --> 00:23:30.220
There was David Attenborough.

00:23:30.220 --> 00:23:32.230
There was James Burke,
and a lot of other men--

00:23:32.230 --> 00:23:33.520
and they were all men--

00:23:33.520 --> 00:23:39.380
who were all giving authority
with the BBC to back them up.

00:23:39.380 --> 00:23:41.570
Which is certainly true,
but for every Planet Earth

00:23:41.570 --> 00:23:43.580
or Connections,
somewhere on the channel

00:23:43.580 --> 00:23:46.360
there would be an Ancient
Mysteries or an In Search Of

00:23:46.360 --> 00:23:48.620
or Ancient Aliens.

00:23:48.620 --> 00:23:51.260
The Bible Code became a
phenomenon in the 1990s.

00:23:51.260 --> 00:23:54.260
This was the idea of there
were certain hidden messages

00:23:54.260 --> 00:23:56.990
in the scriptures that could
be tracked, if you just

00:23:56.990 --> 00:23:58.730
went through it exactly
the right number

00:23:58.730 --> 00:23:59.990
of letters in a row.

00:23:59.990 --> 00:24:01.512
The Daily Mail loved that.

00:24:01.512 --> 00:24:03.470
They put it right on
their front page, a splash

00:24:03.470 --> 00:24:04.620
above the headline.

00:24:04.620 --> 00:24:06.770
Not because it was
true, but because they

00:24:06.770 --> 00:24:08.810
knew it'd sell newspapers.

00:24:08.810 --> 00:24:09.710
We had gatekeepers.

00:24:09.710 --> 00:24:11.210
We definitely did
have gatekeepers,

00:24:11.210 --> 00:24:15.350
but I'm not convinced that
from the public's perspective

00:24:15.350 --> 00:24:17.810
it was all that different
to what we have today.

00:24:17.810 --> 00:24:22.227
Authority online often comes
from having an audience.

00:24:22.227 --> 00:24:24.560
You know, why am I standing
here giving this talk today?

00:24:24.560 --> 00:24:28.610
It's not because I've done 15
years of painstaking research

00:24:28.610 --> 00:24:30.740
into this stuff and
that I am now presenting

00:24:30.740 --> 00:24:32.377
my doctoral thesis to you.

00:24:32.377 --> 00:24:34.460
It's not because I have a
particularly broad range

00:24:34.460 --> 00:24:35.630
of knowledge or
depth of knowledge.

00:24:35.630 --> 00:24:38.150
It's because I've worked with
the Royal Institution before.

00:24:38.150 --> 00:24:39.817
They, you know-- you
know I can present.

00:24:39.817 --> 00:24:42.140
I wouldn't pretend to know
your motivations, Shaun,

00:24:42.140 --> 00:24:44.750
but I think it's
safe to say that I'm

00:24:44.750 --> 00:24:47.508
partly here because you
knew you could sell tickets

00:24:47.508 --> 00:24:49.550
and you knew you'd get
some clicks online from it

00:24:49.550 --> 00:24:51.740
for having me
associated with this.

00:24:51.740 --> 00:24:53.180
I'm not saying that's all of it.

00:24:53.180 --> 00:24:55.400
I'm saying that was
probably a consideration.

00:24:55.400 --> 00:24:58.880
There is a reason that every
major British documentary

00:24:58.880 --> 00:25:00.740
about space for
the last 15 years

00:25:00.740 --> 00:25:03.160
has been presented by
Professor Brian Cox.

00:25:03.160 --> 00:25:05.635
[LAUGHTER]

00:25:07.120 --> 00:25:09.220
Except it's not just
documentaries about space.

00:25:09.220 --> 00:25:10.240
That's Wonders of Life.

00:25:10.240 --> 00:25:11.850
That was about natural history.

00:25:11.850 --> 00:25:14.950
I'm sorry, I'm just
to take a moment.

00:25:14.950 --> 00:25:15.450
Oh, good.

00:25:15.450 --> 00:25:16.350
There he is.

00:25:16.350 --> 00:25:17.023
[LAUGHTER]

00:25:17.023 --> 00:25:19.440
That joke would've landed a
lot better if that had come up

00:25:19.440 --> 00:25:20.273
with the right time.

00:25:20.273 --> 00:25:20.965
[LAUGHTER]

00:25:20.965 --> 00:25:22.590
Can we just take a
minute to appreciate

00:25:22.590 --> 00:25:24.240
how bad the Photoshop
job is, by the way?

00:25:24.240 --> 00:25:25.140
The lighting doesn't match.

00:25:25.140 --> 00:25:27.265
He's got a halo around his
head, and that right arm

00:25:27.265 --> 00:25:29.870
isn't even cut out properly.

00:25:29.870 --> 00:25:32.440
It's an official
BBC Press Photo.

00:25:32.440 --> 00:25:35.070
Anyway, this was billed as
a physics-based approach

00:25:35.070 --> 00:25:36.210
to natural history.

00:25:36.210 --> 00:25:38.800
And yes, it did cover some
pretty advanced physics

00:25:38.800 --> 00:25:40.800
concepts, but let's be
honest, from a television

00:25:40.800 --> 00:25:43.890
commissioner's perspective,
it's a bit of a reach.

00:25:43.890 --> 00:25:45.570
It was almost
certainly an excuse

00:25:45.570 --> 00:25:48.570
to provide Brian
Cox on television

00:25:48.570 --> 00:25:51.030
to the audience who wanted
him, to get those ratings in.

00:25:51.030 --> 00:25:52.730
I'm sure it was well-researched.

00:25:52.730 --> 00:25:54.480
I'm sure they did a
really good job of it.

00:25:54.480 --> 00:25:58.380
But Wonders of Life with
the extremely qualified

00:25:58.380 --> 00:26:02.370
dual biologist an
astrophysicist no one's heard of

00:26:02.370 --> 00:26:05.020
would not have had as
many people watching it.

00:26:05.020 --> 00:26:06.900
It would've been much
harder to commission.

00:26:06.900 --> 00:26:09.300
How much is it
worth to get someone

00:26:09.300 --> 00:26:12.570
less qualified but
known to the audience

00:26:12.570 --> 00:26:15.130
rather than the best
person for the job?

00:26:15.130 --> 00:26:18.670
Sister Wendy was not
some great art historian.

00:26:18.670 --> 00:26:20.230
Her degree was in English.

00:26:20.230 --> 00:26:25.210
But in the 1990s, she presented
five BBC series on art history

00:26:25.210 --> 00:26:28.330
because audiences liked her
and would listen to her.

00:26:28.330 --> 00:26:30.580
In her obituary,
The New York Times

00:26:30.580 --> 00:26:33.940
said, "Her insightful,
unscripted commentaries

00:26:33.940 --> 00:26:38.460
connected emotionally
with millions."

00:26:38.460 --> 00:26:40.770
Danny Beck, a Norwegian
neuroscientist,

00:26:40.770 --> 00:26:41.790
recently said this.

00:26:41.790 --> 00:26:45.300
"Your desire for a platform or
interest in several sciences

00:26:45.300 --> 00:26:48.750
should not supersede your
responsibility and ethical duty

00:26:48.750 --> 00:26:52.020
to not speak on topics
you are not qualified to.

00:26:52.020 --> 00:26:54.690
There exist others who know
better and can do the job.

00:26:54.690 --> 00:26:57.570
Consider amplifying
their voice instead."

00:26:57.570 --> 00:27:00.070
And I agree with that, which
is why the rest of this lecture

00:27:00.070 --> 00:27:01.362
will be present-- no, it won't.

00:27:01.362 --> 00:27:01.954
It won't.

00:27:01.954 --> 00:27:02.665
[LAUGHTER]

00:27:02.665 --> 00:27:04.290
But just for a moment,
you believed me,

00:27:04.290 --> 00:27:05.850
and how did you
feel about paying

00:27:05.850 --> 00:27:07.970
to come here and see that?

00:27:07.970 --> 00:27:10.680
Like the early videos
in my educational series

00:27:10.680 --> 00:27:13.530
were the stereotypical guy
who hasn't done his research

00:27:13.530 --> 00:27:15.990
and thinks he knows everything
spouting unsourced facts.

00:27:15.990 --> 00:27:18.270
There are still a heck
of a lot of people

00:27:18.270 --> 00:27:19.950
who stand in front
of a green screen

00:27:19.950 --> 00:27:22.560
or use voiceover
and stock footage

00:27:22.560 --> 00:27:25.050
and provide no citations
and no references

00:27:25.050 --> 00:27:27.660
and just ask people
to trust them.

00:27:27.660 --> 00:27:29.640
I learned quickly
not to do that,

00:27:29.640 --> 00:27:34.520
but the trouble is, it's sort
of what the audience want.

00:27:34.520 --> 00:27:37.370
A loyal audience, whether
it's on YouTube or Facebook

00:27:37.370 --> 00:27:41.750
or Twitter, does not want to see
someone else's voice amplified.

00:27:41.750 --> 00:27:43.430
If I hit the retweet
button on Twitter,

00:27:43.430 --> 00:27:45.347
I'd just take someone
else's message wholesale

00:27:45.347 --> 00:27:48.470
with their face, I'd send
it out to my audience,

00:27:48.470 --> 00:27:50.360
few people will pass it onwards.

00:27:50.360 --> 00:27:53.420
If I quote tweet it,
if I take their message

00:27:53.420 --> 00:27:56.450
and I add a bit of useless
commentary around it

00:27:56.450 --> 00:27:59.150
with my face, that
goes much further.

00:27:59.150 --> 00:28:00.200
Way more people interact.

00:28:00.200 --> 00:28:02.000
Way more people pass it on.

00:28:02.000 --> 00:28:05.030
YouTube gives creators
retention statistics.

00:28:05.030 --> 00:28:10.190
We get to see, on average, where
people stay and leave a video.

00:28:10.190 --> 00:28:13.286
I will note that that
chart does go up to 120%.

00:28:13.286 --> 00:28:15.480
[LAUGHTER]

00:28:15.480 --> 00:28:17.080
Couple of reasons for that.

00:28:17.080 --> 00:28:21.270
One is that sometimes people
go back and watch a bit again.

00:28:21.270 --> 00:28:22.160
That counts twice.

00:28:22.160 --> 00:28:22.950
That can go above.

00:28:22.950 --> 00:28:26.730
But also, it's because
YouTube's graphing API just

00:28:26.730 --> 00:28:28.226
isn't good at percentages.

00:28:28.226 --> 00:28:30.460
[LAUGHTER]

00:28:30.460 --> 00:28:33.477
There's a big drop off in the
first few seconds up there.

00:28:33.477 --> 00:28:36.060
It's as people go, oh, I don't
like that, or I don't like him,

00:28:36.060 --> 00:28:37.290
or this isn't what I thought.

00:28:37.290 --> 00:28:39.210
There's a big drop
off in the end, what's

00:28:39.210 --> 00:28:41.970
now called the end card and
was once called the credits.

00:28:41.970 --> 00:28:44.160
But apart from that,
just steady slope

00:28:44.160 --> 00:28:45.860
downwards as people get bored.

00:28:45.860 --> 00:28:48.150
And the algorithm will
look kindly on you

00:28:48.150 --> 00:28:51.150
if that slope is less steep.

00:28:51.150 --> 00:28:54.280
But if I go
somewhere on location

00:28:54.280 --> 00:28:56.370
and I hand over to
experts and experts

00:28:56.370 --> 00:28:59.280
carry most of the video,
that graph will be steeper

00:28:59.280 --> 00:29:01.410
and retention will be lower.

00:29:01.410 --> 00:29:03.630
People will get
bored more quickly,

00:29:03.630 --> 00:29:05.730
and they'll show the video less.

00:29:05.730 --> 00:29:07.800
So how much do you
pander to your audience?

00:29:07.800 --> 00:29:10.770
How much do you tell them what
they want to hear, particularly

00:29:10.770 --> 00:29:12.570
when on YouTube
there is literally

00:29:12.570 --> 00:29:16.050
a dollar amount
attached to each video

00:29:16.050 --> 00:29:17.910
and attached to each of you?

00:29:17.910 --> 00:29:19.740
How much do you make
what your audience

00:29:19.740 --> 00:29:23.082
wants at a social cost?

00:29:23.082 --> 00:29:24.540
You know, it's not
just the viewers

00:29:24.540 --> 00:29:28.230
being sent down a rabbit
hole of radicalization.

00:29:28.230 --> 00:29:29.700
It's also the creators.

00:29:29.700 --> 00:29:34.740
When you look at clickbait that
brings in numbers and money,

00:29:34.740 --> 00:29:38.730
it can be very, very tempting
to just cut your losses,

00:29:38.730 --> 00:29:41.190
double down on the
clickbait, and accept that,

00:29:41.190 --> 00:29:43.950
well, make money
while it's coming in.

00:29:43.950 --> 00:29:47.190
Make hay while the sun shines,
because in a couple of weeks,

00:29:47.190 --> 00:29:49.970
it could all go away.

00:29:49.970 --> 00:29:53.530
But the light that burns twice
as bright burns half as long.

00:29:53.530 --> 00:29:57.490
In 1988, Jimmy Cauty and
Bill Drummond, The KLF,

00:29:57.490 --> 00:30:00.890
wrote How to Have a Number
One the Easy Way, the Manual.

00:30:00.890 --> 00:30:02.650
They'd had a
novelty pop song hit

00:30:02.650 --> 00:30:04.442
the top of the charts
a few months earlier,

00:30:04.442 --> 00:30:06.820
and The Manual was a
tongue-in-cheek guide

00:30:06.820 --> 00:30:08.530
to success in the
music industry,

00:30:08.530 --> 00:30:12.700
how to achieve a number one with
no money and no musical talent,

00:30:12.700 --> 00:30:16.150
which they said themselves
that they'd managed.

00:30:16.150 --> 00:30:17.650
Almost all of it
is out of date now.

00:30:17.650 --> 00:30:20.025
There's a wonderful section
a couple of chapters in which

00:30:20.025 --> 00:30:22.030
says that by the
mid-90s, someone in Japan

00:30:22.030 --> 00:30:23.080
will have invented
a machine that

00:30:23.080 --> 00:30:25.540
lets you do this all from home
without a recording studio.

00:30:25.540 --> 00:30:27.010
That was right.

00:30:27.010 --> 00:30:28.810
But at the very
start, a little bit

00:30:28.810 --> 00:30:30.700
that here is as
true now as it was

00:30:30.700 --> 00:30:34.410
when it was written, "The
majority of number ones

00:30:34.410 --> 00:30:37.080
are achieved early in the
artist's public career,

00:30:37.080 --> 00:30:39.570
before they've been able to
establish reputations and build

00:30:39.570 --> 00:30:40.710
a solid fan base.

00:30:40.710 --> 00:30:43.290
Most artists are never able
to recover from having one,

00:30:43.290 --> 00:30:45.450
and it becomes the
millstone around their necks

00:30:45.450 --> 00:30:48.210
to which all subsequent
releases are compared.

00:30:48.210 --> 00:30:50.697
Either the artists will be
destroyed in their attempt

00:30:50.697 --> 00:30:52.530
to prove to the world
there are other facets

00:30:52.530 --> 00:30:55.800
to their creativity, or they
succumb willingly and spend

00:30:55.800 --> 00:30:58.550
the rest of their lives as a
travelling freak show pedalling

00:30:58.550 --> 00:31:02.640
in nostalgia for those now
far off carefree days."

00:31:02.640 --> 00:31:03.420
The Cheetah Girls.

00:31:06.280 --> 00:31:08.740
They wrote that 10 years
before the Cheetah Girls.

00:31:08.740 --> 00:31:11.650
They wrote that 10 years,
20 years before YouTube

00:31:11.650 --> 00:31:14.790
came along, and it's exactly
true for people online.

00:31:14.790 --> 00:31:17.270
A number one does
not create a career.

00:31:17.270 --> 00:31:18.850
It can kill a career.

00:31:18.850 --> 00:31:21.250
If you've just got
one hit, all the world

00:31:21.250 --> 00:31:23.530
will want to see
is that one thing.

00:31:23.530 --> 00:31:25.530
A single video does not
make you a YouTube star.

00:31:25.530 --> 00:31:28.060
A single tweet does not
get you a book deal.

00:31:28.060 --> 00:31:31.730
But to be honest, that hasn't
happened in five years anyway.

00:31:31.730 --> 00:31:33.640
A number one just
makes you the person

00:31:33.640 --> 00:31:36.760
who repeats that catch phrase
until everyone is tired of it.

00:31:36.760 --> 00:31:40.120
What you want to do is build
up a catalogue of minor hits

00:31:40.120 --> 00:31:43.040
over time, get a little bit
of respect, hone your trade,

00:31:43.040 --> 00:31:44.800
learn your craft,
and then once you've

00:31:44.800 --> 00:31:48.310
got a bit of an audience, then
you can start aiming upwards.

00:31:48.310 --> 00:31:50.230
Bands and singers in
the music industry

00:31:50.230 --> 00:31:52.990
is actually a
really good analogy

00:31:52.990 --> 00:31:55.600
for how the internet
works right now.

00:31:55.600 --> 00:31:57.160
After all you know
Brian Cox does

00:31:57.160 --> 00:31:59.410
arena tours and world show--

00:31:59.410 --> 00:32:01.930
arena shows and tours
around the world.

00:32:01.930 --> 00:32:04.840
For the folks out there who are
hoping to communicate science

00:32:04.840 --> 00:32:06.910
to the world, it is
like starting a band

00:32:06.910 --> 00:32:10.630
or launching a music career
or going out solo and singing.

00:32:10.630 --> 00:32:14.600
It will not pay the rent for
the first few years or decades,

00:32:14.600 --> 00:32:15.640
or maybe even ever.

00:32:15.640 --> 00:32:17.980
And if you are one of the
lucky ones, if you make it,

00:32:17.980 --> 00:32:20.060
it will not set you up for life.

00:32:20.060 --> 00:32:22.570
And if you are an organisation
trying to get your message

00:32:22.570 --> 00:32:25.750
out-- well, I used to think that
people had a built-in corporate

00:32:25.750 --> 00:32:27.160
baloney detector.

00:32:27.160 --> 00:32:29.560
I used to think that
anything that was put out

00:32:29.560 --> 00:32:32.710
by an institution or by
an advertising agency

00:32:32.710 --> 00:32:36.260
was doomed to fail just
because it wasn't authentic.

00:32:36.260 --> 00:32:38.690
And that's not true.

00:32:38.690 --> 00:32:39.830
That's not true at all.

00:32:39.830 --> 00:32:41.205
You remember what
I said earlier?

00:32:41.205 --> 00:32:47.100
500 hours of video per
minute, not watched, uploaded.

00:32:47.100 --> 00:32:50.140
You know, 82 years of video
content a day, and most of it

00:32:50.140 --> 00:32:54.160
took almost no time
and effort to produce.

00:32:54.160 --> 00:32:57.160
But every one of those videos
has more or less the same

00:32:57.160 --> 00:33:00.760
chance as that project that your
organisation spent six months

00:33:00.760 --> 00:33:02.840
and a million dollars on.

00:33:02.840 --> 00:33:06.410
The most popular recurring
series that I do right now

00:33:06.410 --> 00:33:07.950
is very simple.

00:33:07.950 --> 00:33:10.100
That's sped up, obviously.

00:33:10.100 --> 00:33:12.980
It is a 10 minute,
one take monologue

00:33:12.980 --> 00:33:14.788
to the camera about
computer science.

00:33:14.788 --> 00:33:15.830
The camera does not move.

00:33:15.830 --> 00:33:17.240
The camera does not cut away.

00:33:17.240 --> 00:33:19.908
There are none of those jump
cuts that a lot of people use.

00:33:19.908 --> 00:33:21.950
I can now completely
understand why they do them.

00:33:21.950 --> 00:33:23.340
It's a lot easier.

00:33:23.340 --> 00:33:25.160
There are occasional
graphics, sometimes,

00:33:25.160 --> 00:33:27.980
but mostly it's just
me and the camera.

00:33:27.980 --> 00:33:31.105
And that breaks all the rules
established by television

00:33:31.105 --> 00:33:31.730
when I grew up.

00:33:31.730 --> 00:33:34.460
It breaks all the rules
that corporate media types

00:33:34.460 --> 00:33:37.800
think that they need
to make this work.

00:33:37.800 --> 00:33:41.010
It's not about spectacle.

00:33:41.010 --> 00:33:42.710
It's about people.

00:33:42.710 --> 00:33:46.520
To explain that, I need to talk
about parasocial relationships.

00:33:49.770 --> 00:33:52.140
The term parasocial
relationship was

00:33:52.140 --> 00:33:55.830
invented by these two,
Donald Horton and Richard--

00:33:55.830 --> 00:33:56.520
Wohl?

00:33:56.520 --> 00:33:57.270
"Vohl?"

00:33:57.270 --> 00:33:59.595
Should really have
fact checked that.

00:33:59.595 --> 00:34:01.080
[LAUGHTER]

00:34:01.080 --> 00:34:03.210
The term means that
there is a difference

00:34:03.210 --> 00:34:06.960
between the spectator, the
viewer as we now call them,

00:34:06.960 --> 00:34:10.440
and the performer, what
we now call the creator.

00:34:10.440 --> 00:34:12.510
The spectator is
emotionally invested

00:34:12.510 --> 00:34:14.730
in the person on screen,
but the person on screen

00:34:14.730 --> 00:34:16.800
has no idea that the
spectator exists.

00:34:16.800 --> 00:34:20.130
And if they do, there's
such a power imbalance there

00:34:20.130 --> 00:34:22.764
that it couldn't
possibly be a friendship.

00:34:22.764 --> 00:34:24.639
Parasocial relationships
are not a new thing,

00:34:24.639 --> 00:34:26.550
and neither is turning
them into money.

00:34:26.550 --> 00:34:28.469
Any celebrity that ever
had an official fan

00:34:28.469 --> 00:34:32.179
club with a membership fee
was doing exactly that.

00:34:32.179 --> 00:34:33.810
Actually, in the late
'80s, early '90s,

00:34:33.810 --> 00:34:36.750
there was a fad for celebrities
to set up phone numbers that

00:34:36.750 --> 00:34:40.199
were their personal number
or their personal voicemail,

00:34:40.199 --> 00:34:42.150
and some of them made
a lot of money from it.

00:34:42.150 --> 00:34:43.701
That's Corey Feldman
and Corey Hart.

00:34:43.701 --> 00:34:45.659
And if you think I didn't
track down the advert

00:34:45.659 --> 00:34:47.710
to play it halfway through to
get everyone's attention back,

00:34:47.710 --> 00:34:48.753
you're completely wrong.

00:34:48.753 --> 00:34:49.420
[VIDEO PLAYBACK]

00:34:49.420 --> 00:34:51.480
- You can listen to their
private phone messages

00:34:51.480 --> 00:34:53.730
and get their personal number
where you can leave them

00:34:53.730 --> 00:34:55.909
a message of your own.
$2 the first minute,

00:34:55.909 --> 00:34:57.850
$0.45 each additional minute.

00:34:57.850 --> 00:34:59.280
Ask your parents
before you call.

00:34:59.280 --> 00:35:00.120
[END PLAYBACK]

00:35:00.120 --> 00:35:01.492
Yeah.

00:35:01.492 --> 00:35:03.347
[LAUGHTER]

00:35:03.347 --> 00:35:04.430
Haven't tested the number.

00:35:04.430 --> 00:35:06.830
I don't imagine
it works anymore.

00:35:06.830 --> 00:35:09.230
But those celebrities
didn't have Twitter.

00:35:09.230 --> 00:35:11.900
Twitter is effectively
someone's personal number.

00:35:11.900 --> 00:35:14.512
If you know that a celebrity
is always on their phone,

00:35:14.512 --> 00:35:16.970
always typing on their computer,
sending their thoughts out

00:35:16.970 --> 00:35:19.767
to the world, well,
why not reply?

00:35:19.767 --> 00:35:20.600
Send them a message.

00:35:20.600 --> 00:35:21.620
They might notice it.

00:35:21.620 --> 00:35:23.960
They might actually
reply to you, personally.

00:35:23.960 --> 00:35:25.910
You know, you might get
attention from them.

00:35:25.910 --> 00:35:28.160
And suddenly it's not a weird
parasocial relationship.

00:35:28.160 --> 00:35:29.300
They're your friend.

00:35:29.300 --> 00:35:32.143
More than that, because
it's on social media,

00:35:32.143 --> 00:35:33.560
you can try to get
that attention.

00:35:33.560 --> 00:35:36.650
You can try to get that
over the top fandom.

00:35:36.650 --> 00:35:38.195
But it's also performative.

00:35:38.195 --> 00:35:39.320
It can also be competitive.

00:35:39.320 --> 00:35:42.062
All the fans can now see
each other doing this.

00:35:42.062 --> 00:35:43.520
I have a couple of
friends who have

00:35:43.520 --> 00:35:47.510
that terrifying
Beatlemania-esque fandom,

00:35:47.510 --> 00:35:50.480
that kids screaming
at Take That concerts,

00:35:50.480 --> 00:35:54.830
except they're not just
folks around magazines

00:35:54.830 --> 00:35:56.420
now in small groups.

00:35:56.420 --> 00:35:58.220
They're not kids who
come for a concert

00:35:58.220 --> 00:36:00.380
and scream at their
idols and then disperse.

00:36:00.380 --> 00:36:03.410
They have notifications on for
when their idol tweets so they

00:36:03.410 --> 00:36:04.760
can get the first reply in.

00:36:04.760 --> 00:36:07.208
They have group chats
whose only distinguishing

00:36:07.208 --> 00:36:08.750
characteristic for
the people in that

00:36:08.750 --> 00:36:11.810
is that they're all
fans of this person.

00:36:11.810 --> 00:36:15.140
If you've ever wondered why
kids would rather sit and watch

00:36:15.140 --> 00:36:18.190
a stream of someone
playing a video game

00:36:18.190 --> 00:36:19.940
rather than just play
the game themselves,

00:36:19.940 --> 00:36:22.080
it's not about the game.

00:36:22.080 --> 00:36:23.480
It's about the
person playing it.

00:36:23.480 --> 00:36:26.760
A stream of a video game on
its own isn't interesting,

00:36:26.760 --> 00:36:29.420
but someone you know or
someone you think you

00:36:29.420 --> 00:36:33.410
know playing a video game,
just hanging out with a friend,

00:36:33.410 --> 00:36:35.785
with a chat next to it.

00:36:35.785 --> 00:36:38.180
In that chat, that's a lot
of friends hanging out.

00:36:38.180 --> 00:36:40.070
You're all just
hanging out together,

00:36:40.070 --> 00:36:43.220
except that one of you has a
lot more power and influence

00:36:43.220 --> 00:36:48.390
than the others and often is
indirectly asking for money.

00:36:48.390 --> 00:36:51.320
You'll notice that I'm not using
any slides during this bit.

00:36:51.320 --> 00:36:54.870
I don't want to call out any
particular individuals for what

00:36:54.870 --> 00:36:56.630
is basically just hustling.

00:36:56.630 --> 00:37:00.060
Patreon and Twitch subscriptions
and YouTube memberships,

00:37:00.060 --> 00:37:02.820
all these tools they
have to raise money

00:37:02.820 --> 00:37:05.900
for individual people are
not inherently a bad thing.

00:37:05.900 --> 00:37:08.490
Patreon has meant that
science communicators are

00:37:08.490 --> 00:37:10.440
able to support themselves
despite the fact

00:37:10.440 --> 00:37:13.830
that sometimes their content
is not advertiser-friendly.

00:37:13.830 --> 00:37:16.530
So people who are talking about
sex education or mental health

00:37:16.530 --> 00:37:20.160
or ancient weapons can all
get money for their content

00:37:20.160 --> 00:37:24.750
and perhaps even hold-- not have
to hold down a separate job,

00:37:24.750 --> 00:37:26.790
because individuals
out in the world

00:37:26.790 --> 00:37:29.780
have thought this
should exist, and I

00:37:29.780 --> 00:37:32.580
am willing to donate
money to make that happen.

00:37:32.580 --> 00:37:35.520
Animators, writers,
podcasters, the sorts

00:37:35.520 --> 00:37:37.950
of people who work
incredibly hard

00:37:37.950 --> 00:37:41.223
and are only able to make
what they do because people

00:37:41.223 --> 00:37:42.390
have chosen to support them.

00:37:42.390 --> 00:37:44.330
That is a brilliant thing.

00:37:44.330 --> 00:37:46.290
But when it starts
to get unsettling,

00:37:46.290 --> 00:37:48.770
when it starts to get
a little bit weird,

00:37:48.770 --> 00:37:51.420
is when it becomes not about
supporting someone's craft,

00:37:51.420 --> 00:37:53.590
but about selling friendship.

00:37:53.590 --> 00:37:56.950
And if you think it's weird
when I put it that way,

00:37:56.950 --> 00:37:57.840
yeah, you should.

00:37:57.840 --> 00:37:59.470
It's really weird.

00:37:59.470 --> 00:38:01.810
If you watch one of the
really popular video

00:38:01.810 --> 00:38:04.450
game streamers on Twitch,
which-- the platform that is

00:38:04.450 --> 00:38:06.310
just video game
streaming, you'll

00:38:06.310 --> 00:38:09.270
see that they're
almost always talking.

00:38:09.270 --> 00:38:10.480
They're watching the chat.

00:38:10.480 --> 00:38:12.220
They're reacting to the
messages that are coming in.

00:38:12.220 --> 00:38:13.120
They're reading them out loud.

00:38:13.120 --> 00:38:13.870
They're replying to them.

00:38:13.870 --> 00:38:15.610
They're calling out the
names that they've seen.

00:38:15.610 --> 00:38:17.260
They're greeting people
who've been hanging around

00:38:17.260 --> 00:38:19.000
in that group for
a long, long time.

00:38:19.000 --> 00:38:23.110
They are being friendly and
open and on for hours at a time,

00:38:23.110 --> 00:38:26.140
performing exhausting
emotional labour.

00:38:26.140 --> 00:38:28.510
They will thank anyone
who sends them a tip,

00:38:28.510 --> 00:38:31.600
or even better yet, subscribes,
because in a world where

00:38:31.600 --> 00:38:34.600
Netflix cost $13 a
month, subscriptions

00:38:34.600 --> 00:38:41.200
to a single person on Twitch
can be $5 or $10 or $25 a month.

00:38:41.200 --> 00:38:43.930
And depending on
what you pay, that

00:38:43.930 --> 00:38:46.520
might affect what
perks you get back

00:38:46.520 --> 00:38:48.880
and what attention you get.

00:38:48.880 --> 00:38:52.000
And if someone does repeat
their subscription, join again,

00:38:52.000 --> 00:38:54.520
then they can choose to
announce it to the whole stream,

00:38:54.520 --> 00:38:56.978
little animation says how long
they've been subscribed for.

00:38:56.978 --> 00:38:59.170
You'll see people on
Twitch say, hey, so-and-so,

00:38:59.170 --> 00:39:02.350
thanks for being part of
the cult for four years.

00:39:02.350 --> 00:39:05.500
That's literally language I
heard while researching this.

00:39:05.500 --> 00:39:10.260
That seems normal to anyone
embedded in that culture.

00:39:10.260 --> 00:39:13.590
Now, if you're a
science communicator

00:39:13.590 --> 00:39:15.340
it won't be quite that much.

00:39:15.340 --> 00:39:17.760
But it might get you
behind the scenes access.

00:39:17.760 --> 00:39:20.447
You know, you might get to see
someone's videos early and put

00:39:20.447 --> 00:39:22.530
some comments in, or get
your name in the credits.

00:39:22.530 --> 00:39:24.405
It might give you access
to a special members

00:39:24.405 --> 00:39:26.730
only private chat room, or
if you're giving someone

00:39:26.730 --> 00:39:28.620
maybe $50 a month,
maybe there'll

00:39:28.620 --> 00:39:30.750
be a private video
chat with the creator,

00:39:30.750 --> 00:39:34.300
just for the folks who are
spending that much money.

00:39:34.300 --> 00:39:35.512
Maybe that is OK.

00:39:35.512 --> 00:39:37.720
Maybe that's the way that
social norms are going now,

00:39:37.720 --> 00:39:39.940
and I'm the old guy looking
at that going, what on earth

00:39:39.940 --> 00:39:40.732
are the kids up to?

00:39:40.732 --> 00:39:42.490
That might be the case.

00:39:42.490 --> 00:39:46.335
But it can be essentially
selling friendship.

00:39:46.335 --> 00:39:47.710
And again, it
doesn't have to be.

00:39:47.710 --> 00:39:49.210
There are people
who use it purely--

00:39:49.210 --> 00:39:50.920
a lot of people who
use it purely as a way

00:39:50.920 --> 00:39:51.837
of funding their work.

00:39:51.837 --> 00:39:54.340
But here is some of
the advice that Patreon

00:39:54.340 --> 00:39:57.040
gives on how to get
more people signed up

00:39:57.040 --> 00:39:59.460
for your monthly subscriptions.

00:39:59.460 --> 00:40:02.970
"Bring your audience along
for the ride by sharing pics,

00:40:02.970 --> 00:40:06.410
videos, and anecdotes
from your life.

00:40:06.410 --> 00:40:08.330
Get vulnerable, within reason.

00:40:08.330 --> 00:40:10.700
An emotional connection
to you as the creator

00:40:10.700 --> 00:40:14.290
can be key in converting
a fan to a patron."

00:40:14.290 --> 00:40:18.310
An emotional connection
can be key in converting

00:40:18.310 --> 00:40:20.080
a fan to a patron.

00:40:20.080 --> 00:40:23.230
There is a very,
very blurred line

00:40:23.230 --> 00:40:25.690
between being a fan
of someone's work

00:40:25.690 --> 00:40:27.730
and being a fan of someone.

00:40:27.730 --> 00:40:30.930
And that's a line I find
really uncomfortable,

00:40:30.930 --> 00:40:32.590
because in part,
my brain doesn't

00:40:32.590 --> 00:40:34.180
do parasocial relationships.

00:40:34.180 --> 00:40:34.920
It never has.

00:40:34.920 --> 00:40:36.650
Maybe there's something
wrong up here.

00:40:36.650 --> 00:40:38.470
But there is, to me,
a huge distinction

00:40:38.470 --> 00:40:41.110
between I'm a fan
of X's work and I'm

00:40:41.110 --> 00:40:43.510
a fan of X. I know
linguistically they

00:40:43.510 --> 00:40:47.620
can be shortened, but those are,
to me, very separate concepts.

00:40:47.620 --> 00:40:50.800
I like the work of Derren
Brown, mentalist, magician.

00:40:50.800 --> 00:40:54.400
I have shamelessly cribbed
some of the techniques

00:40:54.400 --> 00:40:57.070
that I use in talks,
some of the tricks I--

00:40:57.070 --> 00:41:00.177
not like magic tricks, but
some of the rhetorical tricks,

00:41:00.177 --> 00:41:01.510
shamelessly from his live shows.

00:41:01.510 --> 00:41:03.130
The idea of setting
something up at the start

00:41:03.130 --> 00:41:05.050
and letting the audience forget
it and then bring it back

00:41:05.050 --> 00:41:07.467
at the end and revealing it
is the key to the whole thing.

00:41:07.467 --> 00:41:10.360
I have blatantly ripped off
Derren Brown more than once.

00:41:10.360 --> 00:41:13.700
I thoroughly enjoy his work,
and he's a great entertainer.

00:41:13.700 --> 00:41:15.280
But I don't give a
damn about the man

00:41:15.280 --> 00:41:17.230
himself, because
I don't know him.

00:41:17.230 --> 00:41:18.490
He's a stranger.

00:41:18.490 --> 00:41:21.910
Parasocial relationships
and everything about the way

00:41:21.910 --> 00:41:25.360
that these one-way,
one-to-many relationships work,

00:41:25.360 --> 00:41:29.290
blur that line in the
service of greater profit.

00:41:29.290 --> 00:41:32.350
I have a strong memory from
when I was a kid, maybe

00:41:32.350 --> 00:41:36.280
about that tall, being asked
in school to write something

00:41:36.280 --> 00:41:38.590
about a personal hero.

00:41:38.590 --> 00:41:40.440
And I didn't have any.

00:41:40.440 --> 00:41:41.960
With the benefit
of adult hindsight,

00:41:41.960 --> 00:41:44.377
obviously the cop-out option
was to talk about my parents,

00:41:44.377 --> 00:41:49.060
but as the kids around me wrote
about sporting heroes or actors

00:41:49.060 --> 00:41:51.790
or whoever, I just
sat there stumped.

00:41:51.790 --> 00:41:55.360
And it wasn't until I was much
older that I realised to most

00:41:55.360 --> 00:41:58.990
people, liking someone's work
and liking someone are the same

00:41:58.990 --> 00:42:00.170
thing.

00:42:00.170 --> 00:42:02.620
And that was blindingly obvious
to most people, I'm sure,

00:42:02.620 --> 00:42:06.180
but to young me, it
was a revelation.

00:42:06.180 --> 00:42:06.680
There.

00:42:06.680 --> 00:42:08.420
That has revealed something
personal and vulnerable

00:42:08.420 --> 00:42:08.980
within reason.

00:42:08.980 --> 00:42:10.520
That's helping create the
emotional connection--

00:42:10.520 --> 00:42:10.880
[LAUGHTER]

00:42:10.880 --> 00:42:12.180
--between me and the audience.

00:42:12.180 --> 00:42:12.560
Tick.

00:42:12.560 --> 00:42:13.060
[APPLAUSE]

00:42:13.060 --> 00:42:16.274
The collection buckets will
be by the door on the way out.

00:42:16.274 --> 00:42:19.250
[SIGHS] Guys, it's like a cold
breeze just came into the room.

00:42:19.250 --> 00:42:20.220
That was brilliant.

00:42:20.220 --> 00:42:22.740
So why have I covered
that in so much detail?

00:42:22.740 --> 00:42:26.710
What does all that have to do
with science communication?

00:42:26.710 --> 00:42:30.240
It was worked out very early
on in television history

00:42:30.240 --> 00:42:33.150
that the people who were
good at getting an audience

00:42:33.150 --> 00:42:37.010
were not the people who went,
(LOUDLY) ladies and gentlemen.

00:42:37.010 --> 00:42:40.575
They were the people who
went (SOFTLY) hey, hello.

00:42:40.575 --> 00:42:41.950
They weren't the
people who went,

00:42:41.950 --> 00:42:43.655
(LOUDLY) how are we
all doing tonight?

00:42:43.655 --> 00:42:46.030
They were the people who looked
down the camera and said,

00:42:46.030 --> 00:42:47.655
(SOFTLY) how are you?

00:42:47.655 --> 00:42:49.780
It's the difference between
talking to the audience

00:42:49.780 --> 00:42:50.980
and talking to the viewer.

00:42:50.980 --> 00:42:53.860
There is a difference between
a nature documentary with stock

00:42:53.860 --> 00:42:56.800
footage and a voiceover and
a David Attenborough nature

00:42:56.800 --> 00:42:57.460
documentary.

00:42:57.460 --> 00:42:59.440
There is a difference
between Wonders of Life

00:42:59.440 --> 00:43:01.090
and Brian Cox's Wonders of Life.

00:43:01.090 --> 00:43:03.400
Sound quality, factual
accuracy, video quality,

00:43:03.400 --> 00:43:06.220
they all matter, but
not nearly as much

00:43:06.220 --> 00:43:10.720
as having someone on screen who
the audience can connect with.

00:43:10.720 --> 00:43:14.470
My friend Doctor Simon Clarke
vlogged his PhD at Oxford.

00:43:14.470 --> 00:43:16.520
For more old school
people in the audience,

00:43:16.520 --> 00:43:19.540
vlogged essentially
means video diaried.

00:43:19.540 --> 00:43:21.760
Simon now has a doctorate
in atmospheric physics.

00:43:21.760 --> 00:43:24.850
In 2018, he stopped making
personal videos about his life

00:43:24.850 --> 00:43:26.320
in favour of science
communication,

00:43:26.320 --> 00:43:28.960
and he wrote this post
about why and how.

00:43:28.960 --> 00:43:31.870
And I sort of quote
a little bit from it.

00:43:31.870 --> 00:43:34.030
"The motivation of
watching someone struggle

00:43:34.030 --> 00:43:37.090
with the monumental task
of researching a PhD

00:43:37.090 --> 00:43:40.090
was mostly what attracted
viewers to watch.

00:43:40.090 --> 00:43:41.950
I was the product.

00:43:41.950 --> 00:43:44.860
By that, I mean that my
lived existence on earth

00:43:44.860 --> 00:43:50.090
was a commodity, something to
be bottled, refined, and sold."

00:43:50.090 --> 00:43:51.990
Simon's recent science
communication videos

00:43:51.990 --> 00:43:55.420
are really good, but some of his
audience didn't stick around.

00:43:55.420 --> 00:43:57.800
As he changed from
talking about his life

00:43:57.800 --> 00:43:59.480
to talking about
his work, he has

00:43:59.480 --> 00:44:01.220
had to build a new
audience who are

00:44:01.220 --> 00:44:03.540
interested in that
post-academia career of his

00:44:03.540 --> 00:44:05.718
and about the subjects
he's interested in now.

00:44:05.718 --> 00:44:06.510
He's getting there.

00:44:06.510 --> 00:44:08.540
He's doing really well.

00:44:08.540 --> 00:44:11.110
Doctor Clark is in the
minority, because he's qualified

00:44:11.110 --> 00:44:12.110
to talk about a subject.

00:44:12.110 --> 00:44:15.140
For every one of him,
there are countless people

00:44:15.140 --> 00:44:17.420
speculating or repeating
misguided facts

00:44:17.420 --> 00:44:20.930
or just flat-out lying or
trying to shill essential oils

00:44:20.930 --> 00:44:23.060
by claiming they cure cancer.

00:44:23.060 --> 00:44:25.520
You would hope that it's the
people like Doctor Clark that

00:44:25.520 --> 00:44:28.160
would be authoritative, but
often that's not the case.

00:44:28.160 --> 00:44:31.640
Authority frequently comes
from having an audience,

00:44:31.640 --> 00:44:33.380
and having an
audience comes all too

00:44:33.380 --> 00:44:36.560
often from that parasocial
emotional connection

00:44:36.560 --> 00:44:37.220
with people.

00:44:37.220 --> 00:44:40.430
If you are going to try to
talk science to the world

00:44:40.430 --> 00:44:44.670
as an anonymous voice or a
corporation just saying words,

00:44:44.670 --> 00:44:47.000
doesn't really matter how
well-cited your sources are

00:44:47.000 --> 00:44:49.340
or how groundbreaking
your research is.

00:44:49.340 --> 00:44:52.470
You have to tell people about
the human story that's in it,

00:44:52.470 --> 00:44:54.210
that's preferably your own.

00:44:54.210 --> 00:44:58.010
And to a certain extent,
you have to be parasocial.

00:44:58.010 --> 00:45:01.850
So you may think, well, OK, on
television we had gatekeeping.

00:45:01.850 --> 00:45:02.630
We did have that.

00:45:02.630 --> 00:45:04.492
There were certain
standards, surely.

00:45:04.492 --> 00:45:06.700
It may well have been about
parasocial relationships.

00:45:06.700 --> 00:45:09.260
There were occasional failures,
but at least so people we

00:45:09.260 --> 00:45:11.390
could relate to, even if they
weren't technically qualified.

00:45:11.390 --> 00:45:13.355
And again, to be clear,
most of the people who

00:45:13.355 --> 00:45:15.230
are doing this today
are extremely qualified,

00:45:15.230 --> 00:45:17.500
and even if they're not,
there is a whole team

00:45:17.500 --> 00:45:18.750
researching behind the scenes.

00:45:18.750 --> 00:45:23.870
But it's still the medium that
gave us Most Haunted and Ghost

00:45:23.870 --> 00:45:24.673
Hunters.

00:45:24.673 --> 00:45:26.090
And even Most
Haunted, you weren't

00:45:26.090 --> 00:45:27.350
watching because you were
interested in ghosts.

00:45:27.350 --> 00:45:29.475
You were interested in
watching the present [YELPS]

00:45:29.475 --> 00:45:32.820
at something that wasn't there.

00:45:32.820 --> 00:45:34.610
And the online
world is often seen

00:45:34.610 --> 00:45:37.970
as this uncontrolled,
unmediated place where anyone

00:45:37.970 --> 00:45:40.730
can say anything about
anyone, but the last few years

00:45:40.730 --> 00:45:43.610
have shown us that
that's also not the case.

00:45:43.610 --> 00:45:45.560
Which brings me to
the last main part

00:45:45.560 --> 00:45:50.240
of this, which is about
echo chambers and Nazi bars.

00:45:50.240 --> 00:45:52.400
The final piece of the
puzzle, working out

00:45:52.400 --> 00:45:55.010
why some lucky broadcasts
go around the world

00:45:55.010 --> 00:45:58.640
and some don't, is talking about
the people who passed them on.

00:45:58.640 --> 00:46:00.530
It's the groups in
which someone can

00:46:00.530 --> 00:46:03.170
choose to amplify your
voice or condemn it,

00:46:03.170 --> 00:46:04.910
where it's passed
on, person to person

00:46:04.910 --> 00:46:06.785
to person, group
to group to group.

00:46:06.785 --> 00:46:08.660
And some things are
passed on because they're

00:46:08.660 --> 00:46:11.202
interesting or entertaining and
for no more reason than that,

00:46:11.202 --> 00:46:14.870
but often it's because they
support existing views,

00:46:14.870 --> 00:46:16.460
because it's reinforcing
the in-group,

00:46:16.460 --> 00:46:18.710
or because it's
diametrically opposed

00:46:18.710 --> 00:46:20.210
those in-groups of
viewers, and they

00:46:20.210 --> 00:46:22.820
can bond over despising it.

00:46:22.820 --> 00:46:26.510
Up there are the two extremes
of online moderation.

00:46:26.510 --> 00:46:28.940
Let's talk about
the Nazi bar first.

00:46:28.940 --> 00:46:31.490
This is what happens
when a lot of sites

00:46:31.490 --> 00:46:34.190
are set up to be this sort
of bastion of free speech,

00:46:34.190 --> 00:46:35.720
where anything legal--

00:46:35.720 --> 00:46:38.660
and by that, they usually mean
legal under United States law--

00:46:38.660 --> 00:46:40.442
anything legal is free to post.

00:46:40.442 --> 00:46:41.900
And you see this
set up by the sort

00:46:41.900 --> 00:46:45.950
of well-meaning libertarian
tech bros out of Silicon Valley.

00:46:45.950 --> 00:46:48.410
Reddit, which is one of the
major hubs for discussion,

00:46:48.410 --> 00:46:50.500
at least among tech
savvy Americans,

00:46:50.500 --> 00:46:52.220
is perhaps the
canonical example.

00:46:52.220 --> 00:46:55.730
They were set up as this
perfect bulwark of free speech.

00:46:55.730 --> 00:46:58.038
If it is legal, Reddit
will let you say it.

00:46:58.038 --> 00:47:00.080
Small groups in there
might have their own rules,

00:47:00.080 --> 00:47:01.860
but other than that
it is a meritocracy.

00:47:01.860 --> 00:47:04.090
The best ideas will
rise to the top.

00:47:04.090 --> 00:47:06.913
At which point, inevitably,
the Nazis moved in.

00:47:06.913 --> 00:47:08.080
And that's not just a label.

00:47:08.080 --> 00:47:10.538
I'm not just slandering people
with right-wing views there.

00:47:10.538 --> 00:47:13.630
I mean literal
modern day neo-Nazis.

00:47:13.630 --> 00:47:16.430
And unlike most-- like
many European countries,

00:47:16.430 --> 00:47:18.830
the US does not have a
law against incitement

00:47:18.830 --> 00:47:22.220
to religious or racial
hatred, so that was legal.

00:47:22.220 --> 00:47:24.190
So Reddit let them play.

00:47:24.190 --> 00:47:25.940
Free speech, they said,
could be countered

00:47:25.940 --> 00:47:26.900
with more free speech.

00:47:26.900 --> 00:47:29.840
As you might expect, that
lasted until it starts

00:47:29.840 --> 00:47:32.300
to affect their bottom line.

00:47:32.300 --> 00:47:35.060
When there was, finally, when
advertisers were starting

00:47:35.060 --> 00:47:36.602
to have trouble with
them, then there

00:47:36.602 --> 00:47:40.070
was finally a crackdown on the
overtly, staggeringly racist

00:47:40.070 --> 00:47:42.320
discussion there, this
was the quote from one

00:47:42.320 --> 00:47:44.780
of Reddit's co-founders, and
it's kind of astonishing.

00:47:44.780 --> 00:47:46.763
"We didn't ban them
for being racist.

00:47:46.763 --> 00:47:49.430
We banned them because we had to
spend a disproportionate amount

00:47:49.430 --> 00:47:50.750
of time dealing with them."

00:47:50.750 --> 00:47:52.638
[LAUGHTER]

00:47:54.530 --> 00:47:56.330
This isn't exclusive
to Reddit, by the way.

00:47:56.330 --> 00:47:59.990
Facebook's moderation has been
similarly lax and inconsistent,

00:47:59.990 --> 00:48:01.880
and somehow they've
mostly got away with it.

00:48:01.880 --> 00:48:04.580
The inevitable
conclusion of let anyone

00:48:04.580 --> 00:48:07.640
say anything is that the
worst people, having finally

00:48:07.640 --> 00:48:09.860
found a place that
will let them in,

00:48:09.860 --> 00:48:13.110
start to drive out the
more careful and cautious.

00:48:13.110 --> 00:48:15.620
So the discussion swings
a little bit more towards

00:48:15.620 --> 00:48:17.787
their views, which means
more moderate people leave,

00:48:17.787 --> 00:48:19.245
so it swings a
little bit that way,

00:48:19.245 --> 00:48:21.710
and the cycle continues and
continues and continues until

00:48:21.710 --> 00:48:26.180
eventually you realise that
either the worst people survive

00:48:26.180 --> 00:48:30.200
or maybe the moderators might
want to kick out the Nazis.

00:48:30.200 --> 00:48:32.920
This is the analogy to the
Nazi bar, the local pub.

00:48:32.920 --> 00:48:34.902
Might be the greatest
place in town,

00:48:34.902 --> 00:48:36.860
but if they let the Nazis
meet in the basement,

00:48:36.860 --> 00:48:38.480
you're not going to
want to go in there,

00:48:38.480 --> 00:48:39.170
or at least you're
not going to tell

00:48:39.170 --> 00:48:40.378
your friends you go in there.

00:48:40.378 --> 00:48:43.040
In 2015 Reddit conduct
a survey of its users,

00:48:43.040 --> 00:48:45.560
they found the number one
reason that users do not

00:48:45.560 --> 00:48:47.950
recommend the site even
though they use it themselves

00:48:47.950 --> 00:48:50.480
is because they want to
avoid exposing friends

00:48:50.480 --> 00:48:53.740
to hate and offensive content.

00:48:53.740 --> 00:48:56.650
So let's look at the other
extreme, the echo chamber.

00:48:56.650 --> 00:48:58.300
For better analysis
of this, I would

00:48:58.300 --> 00:49:00.970
direct the work of Walter
Quattrociocchi and the folks

00:49:00.970 --> 00:49:03.550
he works with from the
Laboratory of Data Science

00:49:03.550 --> 00:49:07.217
and Complexity at Venice's
Ca' Foscari University.

00:49:07.217 --> 00:49:08.800
I've mispronounced
one of those words.

00:49:08.800 --> 00:49:09.640
I don't know which.

00:49:09.640 --> 00:49:13.280
In an echo chamber, there is
definitely not free speech.

00:49:13.280 --> 00:49:14.440
No dissent is allowed.

00:49:14.440 --> 00:49:16.930
You see that in places
like Facebook groups

00:49:16.930 --> 00:49:19.720
for multi-level marketing
schemes and anti-vaxxers

00:49:19.720 --> 00:49:23.050
where everyone has to buy
in to the group's philosophy

00:49:23.050 --> 00:49:25.480
or else be branded
a shill or a hater.

00:49:25.480 --> 00:49:27.550
Anyone with a
dissenting opinion is

00:49:27.550 --> 00:49:30.370
shouted down by a large
crowd, all of whom

00:49:30.370 --> 00:49:32.120
support each other
in their views,

00:49:32.120 --> 00:49:34.790
whether that view happens to
coincide with reality or not.

00:49:34.790 --> 00:49:36.460
And if all dissent
is banned, you

00:49:36.460 --> 00:49:37.750
end up with a similar problem.

00:49:37.750 --> 00:49:41.590
The most obsessed, the most
extreme radical believers,

00:49:41.590 --> 00:49:43.378
chase out the people
who aren't so sure,

00:49:43.378 --> 00:49:44.920
so the discussion,
on average, starts

00:49:44.920 --> 00:49:48.640
to tolerate more extreme
obsession and less dissent.

00:49:48.640 --> 00:49:51.010
And the cycle continues and
continues and continues.

00:49:51.010 --> 00:49:54.922
If those two failure modes
sound similar, they sort of are.

00:49:54.922 --> 00:49:56.380
Both of those
extremes are harmful.

00:49:56.380 --> 00:49:59.740
And note, I am not talking
about political alignments here.

00:49:59.740 --> 00:50:02.950
I am talking about the
extreme edges of the policies

00:50:02.950 --> 00:50:06.130
that either allow anyone
to say anything or allow

00:50:06.130 --> 00:50:07.990
no dissent whatsoever.

00:50:07.990 --> 00:50:10.990
And every major company
that enables discussion

00:50:10.990 --> 00:50:14.170
has to pick where they sit,
somewhere along that scale.

00:50:14.170 --> 00:50:18.370
So this here's a post from a
Florida-based natural medicine

00:50:18.370 --> 00:50:20.420
clinic that is selling
homoeopathic vaccines.

00:50:23.250 --> 00:50:25.720
I've anonymized them as best
I can for obvious reasons.

00:50:25.720 --> 00:50:27.387
Their phone number
shouldn't be up here.

00:50:27.387 --> 00:50:29.830
I think all of us here can
agree that this is dangerous,

00:50:29.830 --> 00:50:31.997
and there will be wide-ranging
views on whether that

00:50:31.997 --> 00:50:33.160
should be legal.

00:50:33.160 --> 00:50:34.960
I have said, they did
get part of it right.

00:50:34.960 --> 00:50:36.850
Those are definitely
safe for ages above 5.

00:50:40.180 --> 00:50:43.445
There is a concept on
Twitter called the ratio.

00:50:43.445 --> 00:50:45.820
It's the number of replies
you get to the number of likes

00:50:45.820 --> 00:50:47.170
to number of tweets.

00:50:47.170 --> 00:50:48.670
If your retweets
number is biggest,

00:50:48.670 --> 00:50:51.100
you have made something that
has resonated and should

00:50:51.100 --> 00:50:54.070
be signal boosted and
sent it out to the world.

00:50:54.070 --> 00:50:55.750
If your likes number's
biggest, you've

00:50:55.750 --> 00:50:58.420
said something heart warming
or personal or emotional

00:50:58.420 --> 00:50:59.920
that people want
to sympathise with,

00:50:59.920 --> 00:51:03.250
but maybe don't want to
send on to their friends.

00:51:03.250 --> 00:51:05.200
If your replies
number is largest,

00:51:05.200 --> 00:51:07.035
like at the bottom
of that tweet,

00:51:07.035 --> 00:51:09.160
you've probably said
something that a lot of people

00:51:09.160 --> 00:51:10.870
disagree with.

00:51:10.870 --> 00:51:14.980
And a response like that
it's called getting ratioed.

00:51:14.980 --> 00:51:18.010
That homoeopathic vaccine
tweet was thoroughly ratioed.

00:51:18.010 --> 00:51:22.720
Those 130 replies are all people
mocking it or occasionally

00:51:22.720 --> 00:51:25.300
clearly laying out why
homoeopathic vaccines

00:51:25.300 --> 00:51:28.000
are a bad thing, along with
the scientific consensus.

00:51:28.000 --> 00:51:30.190
I think we can agree at
the Royal institution,

00:51:30.190 --> 00:51:32.530
it's probably a good idea
for homoeopathic vaccines

00:51:32.530 --> 00:51:34.310
to get some pushback.

00:51:34.310 --> 00:51:35.560
But here's the problem.

00:51:35.560 --> 00:51:39.490
132 people suddenly replying
to that company who usually

00:51:39.490 --> 00:51:41.980
gets literally zero
replies to anything

00:51:41.980 --> 00:51:44.620
is algorithmically
indistinguishable

00:51:44.620 --> 00:51:49.540
from a mass abuse pile on
targeting a vulnerable person.

00:51:49.540 --> 00:51:52.240
If some awful person with a
moderately sized following

00:51:52.240 --> 00:51:54.650
says, hey, I hate that guy.

00:51:54.650 --> 00:51:56.240
Go and mock him.

00:51:56.240 --> 00:51:58.600
Then machine learning systems
cannot tell the difference

00:51:58.600 --> 00:52:02.950
between abuse and a company
getting pushback for selling

00:52:02.950 --> 00:52:04.480
homoeopathic flu shots.

00:52:04.480 --> 00:52:06.250
Any policy decision
that is designed

00:52:06.250 --> 00:52:09.430
to reduce abuse on Twitter to
stop vulnerable people being

00:52:09.430 --> 00:52:12.550
mass targeted, which
is vital and needed,

00:52:12.550 --> 00:52:16.907
is also going to help
the snake oil peddlers.

00:52:16.907 --> 00:52:18.240
I don't know where to draw that.

00:52:18.240 --> 00:52:19.990
Policy decisions about
community standards

00:52:19.990 --> 00:52:22.530
are often seen about
drawing that line somewhere

00:52:22.530 --> 00:52:24.310
between the echo chamber
and the Nazi bar,

00:52:24.310 --> 00:52:27.860
but there's this idea that
if the company just nudges

00:52:27.860 --> 00:52:30.360
the line a little bit that way
and nudges it a bit that way,

00:52:30.360 --> 00:52:34.720
there will be a perfect solution
that keeps everyone happy.

00:52:34.720 --> 00:52:37.170
I'm really sorry to say
it, but it's not true.

00:52:37.170 --> 00:52:39.540
Echo chamber and Nazi bar
are not two polar opposites.

00:52:39.540 --> 00:52:42.990
They are both a gradient, and
they overlap in the middle.

00:52:42.990 --> 00:52:44.700
You cannot choose
the best option.

00:52:44.700 --> 00:52:48.520
You can only choose
the least worst.

00:52:48.520 --> 00:52:50.580
Slight tangent, but I
think that's due in part

00:52:50.580 --> 00:52:53.130
of the centralization of
the web that's happened

00:52:53.130 --> 00:52:54.960
over the last 20 years or so.

00:52:54.960 --> 00:52:57.600
In the early 2000s, every
discussion site out there

00:52:57.600 --> 00:52:59.937
was on a different server
run by a different person,

00:52:59.937 --> 00:53:01.770
maybe in a different
country with completely

00:53:01.770 --> 00:53:02.640
different rules.

00:53:02.640 --> 00:53:05.640
And that way, it sort
of reflected the systems

00:53:05.640 --> 00:53:07.195
we've got now in real life.

00:53:07.195 --> 00:53:08.820
You know, one of
those sites might well

00:53:08.820 --> 00:53:11.680
allow vulgar abuse and
back and forth argument.

00:53:11.680 --> 00:53:14.720
Another of those sites might ban
someone for even mild swearing.

00:53:14.720 --> 00:53:16.970
And this is how it works in
the real world, you know?

00:53:16.970 --> 00:53:19.260
There is a big difference
between the conversation

00:53:19.260 --> 00:53:22.718
that football fans have chanting
as they go into the stadium,

00:53:22.718 --> 00:53:25.260
and you have the conversation
of the hallowed halls the Royal

00:53:25.260 --> 00:53:26.540
Institution.

00:53:26.540 --> 00:53:27.958
I mean, I assume
there is, Shaun.

00:53:27.958 --> 00:53:30.000
I haven't been here after
last Christmas Lecture.

00:53:30.000 --> 00:53:35.700
I guess gets a little bit messy,
but that different register

00:53:35.700 --> 00:53:39.360
and different social norms is
true for a lot of the bubbles

00:53:39.360 --> 00:53:41.430
within Twitter and
Facebook and YouTube.

00:53:41.430 --> 00:53:44.940
You can have community norms
within smaller subgroups,

00:53:44.940 --> 00:53:47.970
but all those subgroups
are centralised

00:53:47.970 --> 00:53:51.960
on platforms run by enormous,
mostly American corporations.

00:53:51.960 --> 00:53:55.020
So the community standards have
to be standardised through all

00:53:55.020 --> 00:53:56.430
of them, and they
have to be what

00:53:56.430 --> 00:53:59.280
the corporation or their
advertisers or VC backers

00:53:59.280 --> 00:54:00.230
will support.

00:54:00.230 --> 00:54:02.220
You know, you cannot
establish massive,

00:54:02.220 --> 00:54:05.352
cross-platform policies that
allow every type of discussion.

00:54:05.352 --> 00:54:06.810
Or in the case of
YouTube comments,

00:54:06.810 --> 00:54:10.030
any type of
discussion whatsoever.

00:54:10.030 --> 00:54:12.385
The problem is that while
the communities can be small,

00:54:12.385 --> 00:54:15.083
the platforms they're on are
too big and too centralised.

00:54:15.083 --> 00:54:17.500
Because, you know, some people
just kind of go on talking.

00:54:17.500 --> 00:54:21.210
Just like, ah, it's a
nice little coffee shop.

00:54:21.210 --> 00:54:22.018
Friendly space.

00:54:22.018 --> 00:54:22.810
Some people I know.

00:54:22.810 --> 00:54:24.390
You know, just I
talk to my friends.

00:54:24.390 --> 00:54:26.420
And then suddenly, boom,
you know, it's this--

00:54:26.420 --> 00:54:28.770
they get this attack
from these other people

00:54:28.770 --> 00:54:32.160
who use Twitter as this
massive shouting match forum.

00:54:32.160 --> 00:54:34.920
They will search out anyone
using this particular hashtag,

00:54:34.920 --> 00:54:36.420
anyone with these
opinions, and they

00:54:36.420 --> 00:54:37.837
will shout at them,
because that's

00:54:37.837 --> 00:54:39.620
the way they use the platform.

00:54:39.620 --> 00:54:41.590
And a few people try
to federate this.

00:54:41.590 --> 00:54:43.510
It's called-- there's
a network called

00:54:43.510 --> 00:54:46.670
Mastodon, which is
basically Twitter

00:54:46.670 --> 00:54:48.220
on several different servers.

00:54:48.220 --> 00:54:50.262
You know, they all have
their own different rules

00:54:50.262 --> 00:54:52.720
and they all talk to each
other, but running a server

00:54:52.720 --> 00:54:55.040
is complex and expensive.

00:54:55.040 --> 00:54:58.060
And joining Twitter
is free and easy.

00:54:58.060 --> 00:54:59.410
Why would you not do that?

00:54:59.410 --> 00:55:02.080
There is something called
Discord, which is the closest,

00:55:02.080 --> 00:55:03.967
I think, there is to
those old web forums,

00:55:03.967 --> 00:55:05.050
those old bulletin boards.

00:55:05.050 --> 00:55:07.720
Each discussion
section is locked off

00:55:07.720 --> 00:55:11.860
and private, and just separated
from the rest of the world.

00:55:11.860 --> 00:55:13.390
Sounds like a great plan.

00:55:13.390 --> 00:55:14.485
Sounds brilliant.

00:55:14.485 --> 00:55:15.010
[LAUGHTER]

00:55:15.010 --> 00:55:16.950
Sounds great.

00:55:16.950 --> 00:55:20.440
And you know, it might work
for a lot of small groups,

00:55:20.440 --> 00:55:25.070
but it still has a
single, federated--

00:55:25.070 --> 00:55:26.120
excuse me, not federated.

00:55:26.120 --> 00:55:30.760
A single sign on across
the whole network.

00:55:30.760 --> 00:55:33.400
Unintended consequences are
rife if you try and play

00:55:33.400 --> 00:55:34.450
about with this stuff.

00:55:34.450 --> 00:55:36.730
YouTube recently had
an algorithm change,

00:55:36.730 --> 00:55:39.820
which, you know, tried to
raise up authoritative voices.

00:55:39.820 --> 00:55:43.160
Suddenly, if you were watching
videos about climate change,

00:55:43.160 --> 00:55:45.160
then you might be sent
to someone like the Royal

00:55:45.160 --> 00:55:49.190
Institution's videos, which are
about the scientific consensus,

00:55:49.190 --> 00:55:51.910
which means that suddenly, all
the people who were climate

00:55:51.910 --> 00:55:55.780
deniers, who were already
entrenched with their views,

00:55:55.780 --> 00:55:58.153
were being sent to videos
like the Royal Institution's.

00:55:58.153 --> 00:56:00.070
And suddenly, underneath
each of those videos,

00:56:00.070 --> 00:56:02.480
there are ill thought out,
unscientific comments.

00:56:02.480 --> 00:56:05.460
And as a viewer, you can just
scroll down a bit and go,

00:56:05.460 --> 00:56:07.260
ah, yeah, are my people.

00:56:07.260 --> 00:56:09.060
They're the ones that are right.

00:56:09.060 --> 00:56:11.910
Just like you might
if the algorithm had

00:56:11.910 --> 00:56:15.870
determined that your
fundamental beliefs were wrong.

00:56:15.870 --> 00:56:17.490
Which brings us
back to the start.

00:56:17.490 --> 00:56:20.032
I'm pretty sure that the person
running that homoeopathic flu

00:56:20.032 --> 00:56:21.510
clinic genuinely
thinks that they

00:56:21.510 --> 00:56:23.650
are doing good for the world.

00:56:23.650 --> 00:56:26.368
Their fundamental beliefs
are at odds with reality,

00:56:26.368 --> 00:56:28.410
but that's never stopped
people believing things.

00:56:28.410 --> 00:56:31.410
People may take what is
just a saline injection,

00:56:31.410 --> 00:56:33.300
and they may think
that it's effective,

00:56:33.300 --> 00:56:37.610
and they may spread flu, and
that is, in worst case, lethal.

00:56:37.610 --> 00:56:40.710
And I'd argue that in a perfect
world, those tech companies

00:56:40.710 --> 00:56:44.640
that facilitate that discussion
have a moral imperative

00:56:44.640 --> 00:56:47.580
to reduce or remove
messages like that.

00:56:47.580 --> 00:56:51.360
But it can't be that clear
cut, because we aren't perfect.

00:56:51.360 --> 00:56:54.900
The people running them
sure as hell aren't perfect.

00:56:54.900 --> 00:56:57.180
Now, ideally, the
algorithms for Facebook

00:56:57.180 --> 00:56:58.360
or for any other YouTube--

00:56:58.360 --> 00:57:00.630
or for YouTube, or
for any other company,

00:57:00.630 --> 00:57:03.720
they would be able to think
a little bit further ahead.

00:57:03.720 --> 00:57:06.450
They'd be able to increase
long-term profits as they go.

00:57:06.450 --> 00:57:08.790
At least, that's what the
corporations would like.

00:57:08.790 --> 00:57:10.740
They'd be able to
understand public relations,

00:57:10.740 --> 00:57:13.260
and they'd be able to
work out what to do.

00:57:13.260 --> 00:57:16.290
And from humanity's
perspective, the ideal algorithm

00:57:16.290 --> 00:57:19.200
would be helping humanity
survive long term.

00:57:19.200 --> 00:57:21.810
It would suppress conspiracy
theories and fake news,

00:57:21.810 --> 00:57:24.240
but it would allow enough
entertainment and nonsense

00:57:24.240 --> 00:57:26.450
that we still pay
attention to it.

00:57:26.450 --> 00:57:29.120
In 1950, Isaac
Asimov wrote a story

00:57:29.120 --> 00:57:30.680
called "The Evitable Conflict."

00:57:30.680 --> 00:57:33.040
It became the last
chapter of I, Robot.

00:57:33.040 --> 00:57:35.120
And it was about
giant supercomputers

00:57:35.120 --> 00:57:36.500
that run the world's economies.

00:57:36.500 --> 00:57:38.220
They were called the machines.

00:57:38.220 --> 00:57:41.098
And in the story, they're not
being perfectly efficient.

00:57:41.098 --> 00:57:42.390
They're not perfectly designed.

00:57:42.390 --> 00:57:44.182
They're making small
errors here and there.

00:57:44.182 --> 00:57:47.300
It turns out-- spoilers
for a book in 1950--

00:57:47.300 --> 00:57:50.922
spoilers, they are programmed
to protect humanity.

00:57:50.922 --> 00:57:52.880
And they know us better
than we know ourselves.

00:57:52.880 --> 00:57:54.740
Little nudge here,
little nudge there,

00:57:54.740 --> 00:57:57.110
then humanity is less
like to destroy itself,

00:57:57.110 --> 00:58:00.200
and the people who believe that
the machines are doing that

00:58:00.200 --> 00:58:03.898
are more likely to be seen
as conspiracy theorists.

00:58:03.898 --> 00:58:05.940
We don't have machine
learning systems like that.

00:58:05.940 --> 00:58:08.970
We don't have that sort of
artificial intelligence.

00:58:08.970 --> 00:58:09.880
Not yet, anyway.

00:58:09.880 --> 00:58:12.150
And you know, we can't
tell a computer programme,

00:58:12.150 --> 00:58:14.820
here are the odds of humanity
surviving into the next century

00:58:14.820 --> 00:58:15.600
and beyond.

00:58:15.600 --> 00:58:16.827
Improve them.

00:58:16.827 --> 00:58:18.660
If we ever do have a
machine learning system

00:58:18.660 --> 00:58:20.880
like that, the, sort of,
super intelligence that

00:58:20.880 --> 00:58:23.857
could, in theory, control
broad strokes of humanity,

00:58:23.857 --> 00:58:25.440
then the world will
be about to change

00:58:25.440 --> 00:58:28.290
so much that fake news will
be the least of our worries.

00:58:28.290 --> 00:58:29.830
If anyone ever does
figure that out,

00:58:29.830 --> 00:58:32.288
I can only hope that its goal
is not to maximise the profit

00:58:32.288 --> 00:58:33.870
of one company.

00:58:33.870 --> 00:58:37.880
But until someone does
work out an algorithm that

00:58:37.880 --> 00:58:40.780
can do all that, it's up to us.

00:58:40.780 --> 00:58:43.880
And I know this is a really
corny note to end on,

00:58:43.880 --> 00:58:46.353
but like, we are that system.

00:58:46.353 --> 00:58:48.770
It's up to us to fact check
things before we pass them on.

00:58:48.770 --> 00:58:50.460
It's up to us to create
things that are honest,

00:58:50.460 --> 00:58:51.680
that don't smudge the truth.

00:58:51.680 --> 00:58:53.810
It's of the few of us
that create and train

00:58:53.810 --> 00:58:56.775
those algorithms to
understand the biases

00:58:56.775 --> 00:58:59.150
and make sure we're not creating
conspiracy rabbit holes.

00:58:59.150 --> 00:59:01.910
And it's up to those
of us who create things

00:59:01.910 --> 00:59:05.330
to manage those commercial
demands of clickbait and drama

00:59:05.330 --> 00:59:08.420
against honesty and truth
and help the world not

00:59:08.420 --> 00:59:09.950
turn into a horrible pit.

00:59:09.950 --> 00:59:13.310
The only algorithm for
truth that we have right

00:59:13.310 --> 00:59:16.080
now is ourselves.

00:59:16.080 --> 00:59:16.920
My name's Tom Scott.

00:59:16.920 --> 00:59:17.753
Thank you very much.

00:59:17.753 --> 00:59:20.900
[APPLAUSE]

